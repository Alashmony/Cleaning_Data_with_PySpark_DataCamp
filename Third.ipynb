{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c55c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:99% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad103c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa2c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      " None \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "hadoop_home = os.environ.get('HADOOP_HOME', None)\n",
    "spark_home = os.environ.get('PYSPARK_PYTHON', None)\n",
    "java_home = os.environ.get('JAVA_HOME', None)\n",
    "print(hadoop_home,'\\n',spark_home,'\\n',java_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1758b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/27 20:28:47 WARN Utils: Your hostname, Alashmony-Lenovo-Z51-70 resolves to a loopback address: 127.0.1.1; using 192.168.1.182 instead (on interface wlp3s0)\n",
      "23/07/27 20:28:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/27 20:28:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate() \n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1626e720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb1084e6f80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d7a97",
   "metadata": {},
   "source": [
    "# 01 DataFrame details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce790d45",
   "metadata": {},
   "source": [
    "## Intro to data cleaning with Apache Spark\n",
    "\n",
    "1. Intro to data cleaning with Apache Spark\n",
    "\n",
    "Welcome to Data Cleaning in Apache Spark with Python. My name is Mike Metzger, I am a Data Engineering Consultant, and I will be your instructor for this course. We will cover what data cleaning is, why it's important, and how to implement it with Spark and Python. Let's get started!\n",
    "\n",
    "2. What is Data Cleaning?\n",
    "\n",
    "In this course, we'll define \"data cleaning\" as preparing raw data for use in processing pipelines. We'll discuss what a pipeline is later on, but for now, it's sufficient to say that data cleaning is a necessary part of any production data system. If your data isn't \"clean\", it's not trustworthy and could cause problems later on. There are many tasks that could fall under the data cleaning umbrella. A few of these include reformatting or replacing text; performing calculations based on the data; and removing garbage or incomplete data.\n",
    "\n",
    "3. Why perform data cleaning with Spark?\n",
    "\n",
    "Most data cleaning systems have two big problems: optimizing performance and organizing the flow of data. A typical programming language (such as Perl, C++, or even standard SQL) may be able to clean data when you have small quantities of data. But consider what happens when you have millions or even billions of pieces of data. Those languages wouldn't be able to process that amount of information in a timely manner. Spark lets you scale your data processing capacity as your requirements evolve. Beyond the performance issues, dealing with large quantities of data requires a process, or pipeline of steps. Spark allows management of many complex tasks within a single framework.\n",
    "\n",
    "4. Data cleaning example\n",
    "\n",
    "Here's an example of cleaning a small data set. We're given a table of names, age in years, and a city. Our requirements are for a DataFrame with first and last name in separate columns, the age in months, and which state the city is in. We also want to remove any rows where the data is out of the ordinary. Using Spark transformations, we can create a DataFrame with these properties and continue processing afterwards.\n",
    "\n",
    "5. Spark Schemas\n",
    "\n",
    "A primary function of data cleaning is to verify all data is in the expected format. Spark provides a built-in ability to validate datasets with schemas. You may have used schemas before with databases or XML; Spark is similar. A schema defines and validates the number and types of columns for a given DataFrame. A schema can contain many different types of fields - integers, floats, dates, strings, and even arrays or mapping structures. A defined schema allows Spark to filter out data that doesn't conform during read, ensuring expected correctness. In addition, schemas also have performance benefits. Normally a data import will try to infer a schema on read - this requires reading the data twice. Defining a schema limits this to a single read operation.\n",
    "\n",
    "6. Example Spark Schema\n",
    "\n",
    "Here is an example schema to the import data from our previous example. First we'll import the pyspark.sql.types library. Next we define the actual StructType list of StructFields, containing an entry for each field in the data. Each StructField consists of a field name, dataType, and whether the data can be null. Once our schema is defined, we can add it into our spark.read.format.load call and process it against our data. The load() method takes two arguments - the filename and a schema. This is where we apply our schema to the data being loaded.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "We've gone over a lot of information regarding data cleaning and the importance of dataframe schemas. Let's put that information to use and practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0293c",
   "metadata": {},
   "source": [
    "### Data cleaning review\n",
    "There are many benefits for using Spark for data cleaning.\n",
    "\n",
    "Which of the following is NOT a benefit?\n",
    "\n",
    "- Spark offers high performance.\n",
    "\n",
    "- **Spark can only handle thousands of records.**\n",
    "\n",
    "- Spark allows orderly data flows.\n",
    "\n",
    "- Spark can use strictly defined schemas while ingesting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c845a",
   "metadata": {},
   "source": [
    "### Defining a schema\n",
    "Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "\n",
    "Name\n",
    "Age\n",
    "City\n",
    "The `Name` and `City` columns are `StringType()` and the `Age` column is an `IntegerType()`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import * from the `pyspark.sql.types` library.\n",
    "- Define a new schema using the `StructType` method.\n",
    "- Define a `StructField` for `name`, `age`, and `city`. Each field should correspond to the correct datatype and not be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12db931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13572b6",
   "metadata": {},
   "source": [
    "## Immutability and lazy processing\n",
    "\n",
    "1. Immutability and Lazy Processing\n",
    "\n",
    "Welcome back! We've had a quick discussion about data cleaning, data types and schemas. Let's move on to some further Spark concepts - Immutability and Lazy Processing.\n",
    "\n",
    "2. Variable review\n",
    "\n",
    "Normally in Python, and most other languages, variables are fully mutable. The values can be changed at any given time, assuming the scope of the variable is valid. While very flexible, this does present problems anytime there are multiple concurrent components trying to modify the same data. Most languages work around these issues using constructs like mutexes, semaphores, etc. This can add complexity, especially with non-trivial programs.\n",
    "\n",
    "3. Immutability\n",
    "\n",
    "Unlike typical Python variables, Spark Data Frames are immutable. While not strictly required, immutability is often a component of functional programming. We won't go into everything that implies here, but understand that Spark is designed to use immutable objects. Practically, this means Spark Data Frames are defined once and are not modifiable after initialization. If the variable name is reused, the original data is removed (assuming it's not in use elsewhere) and the variable name is reassigned to the new data. While this seems inefficient, it actually allows Spark to share data between all cluster components. It can do so without worry about concurrent data objects.\n",
    "\n",
    "4. Immutability Example\n",
    "\n",
    "This is a quick example of the immutability of data frames in Spark. It's OK if you don't understand the actual code, this example is more about the concepts of what happens. First, we create a data frame from a CSV file called voterdata.csv. This creates a new data frame definition and assigns it to the variable name voter_df. Once created, we want to do two further operations. The first is to create a fullyear column by using a 2-digit year present in the data set and adding 2000 to each entry. This does not actually change the data frame at all. It copies the original definition, adds the transformation, and assigns it to the voter_df variable name. Our second operation is similar - now we want to drop the original year column from the data frame. Again, this copies the definition, adds a transformation and reassigns the variable name to this new object. The original objects are destroyed. Please note that the original year column is now permanently gone from this instance, though not from the underlying data (ie, you could simply reload it to a new dataframe if desired).\n",
    "\n",
    "5. Lazy Processing\n",
    "\n",
    "You may be wondering how Spark does this so quickly, especially on large data sets. Spark can do this because of something called lazy processing. Lazy processing in Spark is the idea that very little actually happens until an action is performed. In our previous example, we read a CSV file, added a new column, and deleted another. The trick is that no data was actually read / added / modified, we only updated the instructions (aka, Transformations) for what we wanted Spark to do. This functionality allows Spark to perform the most efficient set of operations to get the desired result. The code example is the same as the previous slide, but with the added count() method call. This classifies as an action in Spark and will process all the transformation operations.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "These concepts can be a little tricky to grasp without some examples. Let's practice these ideas in the coming exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31537d05",
   "metadata": {},
   "source": [
    "### Immutability review\n",
    "You’ve just seen that immutability and lazy processing are fundamental concepts in the way Spark handles data. But why would Spark use immutable data frames to begin with?\n",
    "\n",
    "\n",
    "- To add complexity to your Spark tasks.\n",
    "\n",
    "- **To efficiently handle data throughout the cluster.**\n",
    "\n",
    "- To easily modify variable values as needed.\n",
    "\n",
    "- To conserve storage space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c61745",
   "metadata": {},
   "source": [
    "## Using lazy processing\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (`aa_dfw_df`) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Load the Data Frame.\n",
    "- Add the transformation for `F.lower()` to the Destination Airport column.\n",
    "- Drop the Destination Airport column from the Data Frame `aa_dfw_df`. Note the time for these operations to complete.\n",
    "- Show the Data Frame, noting the time difference for this action to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65882073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2017_Departures_Short.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f410cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39604d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f248dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa8d77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "|       01/01/2017|         0060|                          149|    mia|\n",
      "|       01/01/2017|         0071|                          203|    lax|\n",
      "|       01/01/2017|         0074|                           76|    mem|\n",
      "|       01/01/2017|         0081|                          123|    den|\n",
      "|       01/01/2017|         0089|                          161|    slc|\n",
      "|       01/01/2017|         0096|                           84|    stl|\n",
      "|       01/01/2017|         0103|                          216|    sjc|\n",
      "|       01/01/2017|         0119|                          514|    ogg|\n",
      "|       01/01/2017|         0123|                          529|    hnl|\n",
      "|       01/01/2017|         0126|                          171|    lga|\n",
      "|       01/01/2017|         0132|                          188|    ewr|\n",
      "|       01/01/2017|         0140|                          231|    sjc|\n",
      "|       01/01/2017|         0174|                          145|    rdu|\n",
      "|       01/01/2017|         0176|                          184|    bos|\n",
      "|       01/01/2017|         0190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba834a6",
   "metadata": {},
   "source": [
    "## Understanding Parquet\n",
    "\n",
    "1. Understanding Parquet\n",
    "\n",
    "Welcome back! As we've seen, Spark can read in text and CSV files. While this gives us access to many data sources, it's not always the most convenient format to work with. Let's take a look at a few problems with CSV files.\n",
    "\n",
    "2. Difficulties with CSV files\n",
    "\n",
    "Some common issues with CSV files include: The schema is not defined: there are no data types included, nor column names (beyond a header row). Using content containing a comma (or another delimiter) requires escaping. Using the escape character within content requires even further escaping. The available encoding formats are limited depending on the language used.\n",
    "\n",
    "3. Spark and CSV files\n",
    "\n",
    "In addition to the issues with CSV files in general, Spark has some specific problems processing CSV data. CSV files are quite slow to import and parse. The files cannot be shared between workers during the import process. If no schema is defined, all data must be read before a schema can be inferred. Spark has feature known as predicate pushdown. Basically, this is the idea of ordering tasks to do the least amount of work. Filtering data prior to processing is one of the primary optimizations of predicate pushdown. This drastically reduces the amount of information that must be processed in large data sets. Unfortunately, you cannot filter the CSV data via predicate pushdown. Finally, Spark processes are often multi-step and may utilize an intermediate file representation. These representations allow data to be used later without regenerating the data from source. Using CSV would instead require a significant amount of extra work defining schemas, encoding formats, etc.\n",
    "\n",
    "4. The Parquet Format\n",
    "\n",
    "Parquet is a compressed columnar data format developed for use in any Hadoop based system. This includes Spark, Hadoop, Apache Impala, and so forth. The Parquet format is structured with data accessible in chunks, allowing efficient read / write operations without processing the entire file. This structured format supports Spark's predicate pushdown functionality, providing significant performance improvement. Finally, Parquet files automatically include schema information and handle data encoding. This is perfect for intermediary or on-disk representation of processed data. Note that Parquet files are a binary file format and can only be used with the proper tools. This is in contrast to CSV files which can be edited with any text editor.\n",
    "\n",
    "5. Working with Parquet\n",
    "\n",
    "Interacting with Parquet files is very straightforward. To read a parquet file into a Data Frame, you have two options. The first is using the `spark.read.format` method we've seen previously. The Data Frame, df=spark.read.format('parquet').load('filename.parquet') The second option is the shortcut version: The Data Frame, df=spark.read.parquet('filename.parquet') Typically, the shortcut version is the easiest to use but you can use them interchangeably. Writing parquet files is similar, using either: df.write.format('parquet').save('filename.parquet') or df.write.parquet('filename.parquet') The long-form versions of each permit extra option flags, such as when overwriting an existing parquet file.\n",
    "\n",
    "6. Parquet and SQL\n",
    "\n",
    "Parquet files have various uses within Spark. We've discussed using them as an intermediate data format, but they also are perfect for performing SQL operations. To perform a SQL query against a Parquet file, we first need to create a Data Frame via the spark.read.parquet method. Once we have the Data Frame, we can use the createOrReplaceTempView() method to add an alias of the Parquet data as a SQL table. Finally, we run our query using normal SQL syntax and the spark.sql method. In this case, we're looking for all flights with a duration under 100 minutes. Because we're using Parquet as the backing store, we get all the performance benefits we've discussed previously (primarily defined schemas and the available use of predicate pushdown).\n",
    "\n",
    "7. Let's Practice!\n",
    "\n",
    "You've seen a bit about what a Parquet file is and why we'd want to use them. Now, let's practice working with Parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e0414",
   "metadata": {},
   "source": [
    "## Saving a DataFrame in Parquet format\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it.\n",
    "\n",
    "The spark object and the `df1` and `df2` DataFrames have been setup for you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- View the row count of `df1` and `df2`.\n",
    "- Combine `df1` and `df2` in a new DataFrame named `df3` with the union method.\n",
    "- Save `df3` to a parquet file named `AA_DFW_ALL.parquet`.\n",
    "- Read the `AA_DFW_ALL.parquet` file and show the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a27659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('AA_DFW_2014_Departures_Short.csv.gz', header= True)\n",
    "df2 = spark.read.csv('AA_DFW_2015_Departures_Short.csv.gz', header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1320aa",
   "metadata": {},
   "source": [
    "### If Spark was not able to save as Parquet format, change the winutil version used as Hadoop Home.\n",
    "Mostly, you will need to upgrade WinUtil to match the same Spark version, in my case, Spark 3.4 needs to match Hadoop 3.4. [This repo](https://github.com/kontext-tech/winutils.git) has newer version but not fully trusted. For me, it solved the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247dcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_home = os.environ.get('HADOOP_HOME', None)\n",
    "hadoop_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d206dd70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 157198\n",
      "df2 Count: 146558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303756\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "#rename column to be used in the next cell\n",
    "df3 = df3.withColumnRenamed('Actual elapsed time (Minutes)', 'flight_duration')\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ff654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2014|         0005|                HNL|                          519|\n",
      "|       01/01/2014|         0007|                OGG|                          505|\n",
      "|       01/01/2014|         0035|                SLC|                          174|\n",
      "|       01/01/2014|         0043|                DTW|                          153|\n",
      "|       01/01/2014|         0052|                PIT|                          137|\n",
      "|       01/01/2014|         0058|                SAN|                          174|\n",
      "|       01/01/2014|         0060|                MIA|                          155|\n",
      "|       01/01/2014|         0064|                JFK|                          185|\n",
      "|       01/01/2014|         0090|                ORD|                          126|\n",
      "|       01/01/2014|         0096|                STL|                           91|\n",
      "|       01/01/2014|         0099|                SNA|                          182|\n",
      "|       01/01/2014|         0103|                ONT|                          181|\n",
      "|       01/01/2014|         0109|                DEN|                          127|\n",
      "|       01/01/2014|         0122|                SFO|                          222|\n",
      "|       01/01/2014|         0123|                HNL|                          510|\n",
      "|       01/01/2014|         0129|                COS|                          114|\n",
      "|       01/01/2014|         0130|                DCA|                          141|\n",
      "|       01/01/2014|         0131|                SLC|                          167|\n",
      "|       01/01/2014|         0132|                STL|                           82|\n",
      "|       01/01/2014|         0140|                BWI|                          146|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0439f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2015|         0005|                HNL|                          526|\n",
      "|       01/01/2015|         0007|                OGG|                          517|\n",
      "|       01/01/2015|         0023|                SFO|                          233|\n",
      "|       01/01/2015|         0027|                LAS|                          165|\n",
      "|       01/01/2015|         0029|                ONT|                            0|\n",
      "|       01/01/2015|         0035|                HDN|                          178|\n",
      "|       01/01/2015|         0037|                SAN|                          187|\n",
      "|       01/01/2015|         0043|                DTW|                            0|\n",
      "|       01/01/2015|         0049|                SAN|                          178|\n",
      "|       01/01/2015|         0051|                SLC|                          161|\n",
      "|       01/01/2015|         0060|                MIA|                          151|\n",
      "|       01/01/2015|         0064|                JFK|                          187|\n",
      "|       01/01/2015|         0071|                SAN|                          176|\n",
      "|       01/01/2015|         0072|                MCO|                          142|\n",
      "|       01/01/2015|         0074|                CLE|                            0|\n",
      "|       01/01/2015|         0079|                SMF|                          224|\n",
      "|       01/01/2015|         0081|                TUS|                          140|\n",
      "|       01/01/2015|         0096|                STL|                           94|\n",
      "|       01/01/2015|         0103|                MSY|                           80|\n",
      "|       01/01/2015|         0119|                OGG|                          502|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "848007f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+---------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|flight_duration|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "|       01/01/2014|         0005|                HNL|            519|\n",
      "|       01/01/2014|         0007|                OGG|            505|\n",
      "|       01/01/2014|         0035|                SLC|            174|\n",
      "|       01/01/2014|         0043|                DTW|            153|\n",
      "|       01/01/2014|         0052|                PIT|            137|\n",
      "|       01/01/2014|         0058|                SAN|            174|\n",
      "|       01/01/2014|         0060|                MIA|            155|\n",
      "|       01/01/2014|         0064|                JFK|            185|\n",
      "|       01/01/2014|         0090|                ORD|            126|\n",
      "|       01/01/2014|         0096|                STL|             91|\n",
      "|       01/01/2014|         0099|                SNA|            182|\n",
      "|       01/01/2014|         0103|                ONT|            181|\n",
      "|       01/01/2014|         0109|                DEN|            127|\n",
      "|       01/01/2014|         0122|                SFO|            222|\n",
      "|       01/01/2014|         0123|                HNL|            510|\n",
      "|       01/01/2014|         0129|                COS|            114|\n",
      "|       01/01/2014|         0130|                DCA|            141|\n",
      "|       01/01/2014|         0131|                SLC|            167|\n",
      "|       01/01/2014|         0132|                STL|             82|\n",
      "|       01/01/2014|         0140|                BWI|            146|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25614b",
   "metadata": {},
   "source": [
    "### SQL and Parquet\n",
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file).\n",
    "\n",
    "The spark object and the `AA_DFW_ALL.parquet` file are available for you automatically.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `AA_DFW_ALL.parquet` file into flights_df.\n",
    "- Use the `createOrReplaceTempView` method to alias the flights table.\n",
    "- Run a Spark SQL query against the flights table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32e67bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 143\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7db0e",
   "metadata": {},
   "source": [
    "# Chapter 2: Manipulating DataFrames in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95836c34",
   "metadata": {},
   "source": [
    "## Manipulating DataFrames in the real world\n",
    "1. DataFrame column operations\n",
    "\n",
    "Welcome back! In the first chapter, we've spent some time discussing the basics of Spark data and file handling. Let's now take a look at how to use Spark column operations to clean data.\n",
    "\n",
    "2. DataFrame refresher\n",
    "\n",
    "Before we discuss manipulating DataFrames in depth, let's talk about some of their features. DataFrames are made up of rows & columns and are generally analogous to a database table. DataFrames are immutable: any change to the structure or content of the data creates a new DataFrame. DataFrames are modified through the use of transformations. An example is The .filter() command to only return rows where the name starts with the letter 'M'. Another operation is .select(), in this case returning only the name and position fields.\n",
    "\n",
    "3. Common DataFrame transformations\n",
    "\n",
    "There are many different transformations for use on a DataFrame. They vary depending on what you'd like to do. Some common transformations include: The .filter() clause, which includes only rows that satisfy the requirements defined in the argument. This is analogous to the WHERE clause in SQL. Spark includes a .where() alias which you can use in place of .filter() if desired. This call returns only rows where the vote occurred after 1/1/2019. Another common option is the .select() method which returns the columns requested from the DataFrame. The .withColumn() method creates a new column in the DataFrame. The first argument is the name of the column, and the second is the command(s) to create it. In this case, we create a column called 'year' with just the year information. We also can use the .drop() method to remove a column from a DataFrame.\n",
    "\n",
    "4. Filtering data\n",
    "\n",
    "Among the most common operations used when cleaning a DataFrame, filtering lets us use only the data matching our desired result. We can use .filter() for many tasks, such as: Removing null values. Removing odd entries, anything that doesn't fit our desired format. We can also split a DataFrame containing combined data (such as a syslog file). As mentioned previously, use the .filter() method to return only rows that meet the specified criteria. The .contains() function takes a string argument that the column must have to return true. You can negate these results using the tile (~) character.\n",
    "\n",
    "5. Column string transformations\n",
    "\n",
    "Some of the most common operations used in data cleaning are modifying and converting strings. You will typically apply these to each column as a transformation. Many of these functions are in the pyspark.sql.functions library. For brevity, we'll import it as the alias 'F'. We use the .withColumn() function to create a new column called \"upper\" using pyspark.sql.functions.upper() on the name column. The \"upper\" column will contain uppercase versions of all names. We can create intermediary columns that are only for processing. This is useful to clarify complex transformations requiring multiple steps. In this instance, we call the .split() function with the name of the column and the space character to split on. This returns a list of words in a column called splits. A very common operation is converting string data to a different type, such as converting a string column to an integer. We use the .cast() function to perform the conversion to an IntegerType().\n",
    "\n",
    "6. ArrayType() column functions\n",
    "\n",
    "While performing data cleaning with Spark, you may need to interact with ArrayType() columns. These are analogous to lists in normal python environments. One function we will use is .size(), which returns the number of items present in the specified ArrayType() argument. Another commonly used function for ArrayTypes is .getItem(). It takes an index argument and returns the item present at that index in the list column. Spark has many more transformations and utility functions available. When using Spark in production, make sure to reference the documentation for available options.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "We've discussed some of the common operations used on Spark DataFrame columns. Let's practice some of these now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927f573",
   "metadata": {},
   "source": [
    "### Filtering column content with Python\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame `voter_df` contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the `VOTER_NAME` column.\n",
    "\n",
    "The `pyspark.sql.functions` library is already imported under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Show the distinct `VOTER_NAME` entries.\n",
    "- Filter `voter_df` where the `VOTER_NAME` is 1-20 characters in length.\n",
    "- Filter out `voter_df` where the `VOTER_NAME` contains an `_`.\n",
    "- Show the distinct `VOTER_NAME` entries again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93655d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "voter_df = spark.read.csv('DallasCouncilVoters.csv.gz', header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c72d31e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District              |\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Michael S. Rawlings                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and  the  methods  of  assessing  special  assessments  on  Dallas  hotels  with    100 or more rooms                                                                                                                           |\n",
      "|Kevin Felder                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Adam Medrano                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Casey  Thomas                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|null                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll  (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District |\n",
      "|011018__42                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Mark  Clayton                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Casey Thomas                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Sandy  Greyson                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Mark Clayton                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Jennifer S.  Gates                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Tiffinni A. Young                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|  the  final  2018 Assessment  Plan   and   the   2018 Assessment   Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing   classifications  for the apportionment of costs and the  methods  of  assessing  special  assessments for the services and improvements  to  property  in  the  District;  closing the hearing and  levying  a  special  assessment  on  property  in  the  District       |\n",
      "|B. Adam  McGough                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|Omar Narvaez                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Philip T. Kingston                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Rickey D. Callahan                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Dwaine R. Caraway                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Philip T.  Kingston                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Jennifer S. Gates                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Lee M. Kleinman                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Monica R. Alonzo                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll   (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District|\n",
      "|Rickey D.  Callahan                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Carolyn King Arnold                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District               |\n",
      "|Erik Wilson                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|  the  final  2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District; closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District                 |\n",
      "|Lee Kleinman                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa03129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Casey  Thomas      |\n",
      "|Mark  Clayton      |\n",
      "|Casey Thomas       |\n",
      "|Sandy  Greyson     |\n",
      "|Mark Clayton       |\n",
      "|Jennifer S.  Gates |\n",
      "|Tiffinni A. Young  |\n",
      "|B. Adam  McGough   |\n",
      "|Omar Narvaez       |\n",
      "|Philip T. Kingston |\n",
      "|Rickey D. Callahan |\n",
      "|Dwaine R. Caraway  |\n",
      "|Philip T.  Kingston|\n",
      "|Jennifer S. Gates  |\n",
      "|Lee M. Kleinman    |\n",
      "|Monica R. Alonzo   |\n",
      "|Rickey D.  Callahan|\n",
      "|Carolyn King Arnold|\n",
      "|Erik Wilson        |\n",
      "|Lee Kleinman       |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.where('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfe4b7",
   "metadata": {},
   "source": [
    "### Filtering Question #1\n",
    "Consider the following Data Frame called `users_df`:\n",
    "\n",
    "|ID| Name| Age|\tState|\n",
    "|---|---|---|---|\n",
    "|140|\tGeorge| L|\t47|\tIowa|\n",
    "|3260|\tMary| R|\t34|\tVermont|\n",
    "|18502|\tnull|\t68|\tOhio|\n",
    "|999|\tRick| W|\t23|\tCalifornia|\n",
    "\n",
    "If you wanted to return only the entries without nulls, which of following options would **not** work?\n",
    "\n",
    "**Answer the question**\n",
    "**Possible Answers**\n",
    "\n",
    "- users_df = users_df.filter(users_df.Name.isNotNull())\n",
    "\n",
    "- **users_df = users_df.where(users_df.ID == 18502)**\n",
    "\n",
    "- users_df = users_df.where(~ (users_df.ID == 18502) )\n",
    "\n",
    "- users_df = users_df.filter(~ col('Name').isNull())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f31bc",
   "metadata": {},
   "source": [
    "### Modifying DataFrame columns\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - `first_name` and `last_name`. She asks you to split the `VOTER_NAME` column into words on any space character. You'll treat the last word as the `last_name`, and all other words as the `first_name`. You'll be using some new functions in this exercise including `.split()`, `.size()`, and `.getItem()`. The `.getItem(index)` takes an integer value to return the appropriately numbered item in the column. The functions `.split()` and `.size()` are in the `pyspark.sql.functions` library.\n",
    "\n",
    "Please note that these operations are always somewhat specific to the use case. Having your data conform to a format often matters more than the specific details of the format. Rarely is a data cleaning task meant just for one person - matching a defined format allows for easier sharing of the data later (ie, Paul doesn't need to worry about names - Mary already cleaned the dataset).\n",
    "\n",
    "The filtered voter DataFrame from your previous exercise is available as `voter_df`. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Add a new column called splits holding the list of possible names.\n",
    "- Use the `getItem()` method and create a new column called `first_name`.\n",
    "- Get the last entry of the splits list and create a column called `last_name`.\n",
    "- Drop the splits column and show the new `voter_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9b48dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to import again, just aliassing it\n",
    "F = pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb43f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbd5d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alashmony/.local/lib/python3.10/site-packages/pyspark/sql/column.py:458: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits')-1))\n",
    "\n",
    "# Drop the splits column\n",
    "#voter_df = voter_df.drop('splits') #Hashes as we will need it later\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d397bc3",
   "metadata": {},
   "source": [
    "## Conditional DataFrame column operations\n",
    "\n",
    "1. Conditional DataFrame column operations\n",
    "\n",
    "We've looked at some of the power available when using Spark's functions to filter and modify our Data Frames. Let's spend some time with some more advanced options.\n",
    "\n",
    "2. Conditional clauses\n",
    "\n",
    "The DataFrame transformations we've covered thus far are blanket transformations, meaning they're applied regardless of the data. Often you'll want to conditionally change some aspect of the contents. Spark provides some built in conditional clauses which act similar to an if / then / else statement in a traditional programming environment. While it is possible to perform a traditional if / then / else style statement in Spark, it can lead to serious performance degradation as each row of a DataFrame would be evaluated independently. Using the optimized, built-in conditionals alleviates this. There are two components to the conditional clauses: .when(), and the optional .otherwise(). Let's look at them in more depth.\n",
    "\n",
    "3. Conditional example\n",
    "\n",
    "The .when() clause is a method available from the pyspark.sql.functions library that is looking for two components: the if condition, and what to do if it evaluates to true. This is best seen from an example. Consider a DataFrame with the Name and Age columns. We can actually add an extra argument to our .select() method using the .when() clause. We select df.Name and df.Age as usual. For the third argument, we'll define a when conditional. If the Age column is 18 or up, we'll add the string \"Adult\". If the clause doesn't match, nothing is returned. Note that our returned DataFrame contains an unnamed column we didn't define using .withColumn(). The .select() function can create columns dynamically based on the arguments provided. Let's look at some more examples.\n",
    "\n",
    "4. Another example\n",
    "\n",
    "You can chain multiple when statements together, similar to an if / else if structure. In this case, we define two .when() clauses and return Adult or Minor based on the Age column. You can chain as many when clauses together as required.\n",
    "\n",
    "5. Otherwise\n",
    "\n",
    "In addition to .when() is the otherwise() clause. .otherwise() is analogous to the else statement. It takes a single argument, which is what to return, in case the when clause or clauses do not evaluate as True. In this example, we return \"Adult\" when the Age column is 18 or higher. Otherwise, we return \"Minor\". The resulting DataFrame is the same, but the method is different. While you can have multiple .when() statements chained together, you can only have a single .otherwise() per .when() chain.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "Let's try a couple examples of using .when() and .otherwise() to modify some DataFrames!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca701726",
   "metadata": {},
   "source": [
    "### when() example\n",
    "The `when()` clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our `voter_df` DataFrame to add a random number to any voting member that is defined as a \"Councilmember\".\n",
    "\n",
    "The `voter_df` DataFrame is defined and available to you. The `pyspark.sql.functions` library is available as `F`. You can use `F.rand()` to generate the random value.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Add a column to `voter_df` named `random_val` with the results of the `F.rand()` method for any voter with the title Councilmember.\n",
    "- Show some of the DataFrame rows, noting whether the `.when()` clause worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e81336cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.24106818690281073|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston| 0.6972561362426389|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|               null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano| 0.8943244183077836|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas| 0.4893316529063245|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|  0.969187568357373|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs| 0.3055369254728457|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough| 0.9847399990007926|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman| 0.5152078885745381|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson| 0.8727958459329025|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.4432784998270186|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston| 0.8351206842619017|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|               null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano| 0.8749882060990434|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|  0.682650915921878|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.6815321492733507|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|0.23899577607607636|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.39137589666477557|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|0.35210058252188126|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.7299437089473029|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970db3ef",
   "metadata": {},
   "source": [
    "### When / Otherwise\n",
    "This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your `voter_df` DataFrame to add a random number to any voting member that is defined as a Councilmember. Use 2 for the Mayor and 0 for anything other position.\n",
    "\n",
    "The `voter_df` Data Frame is defined and available to you. The `pyspark.sql.functions` library is available as `F`. You can use `F.rand()` to generate the random value.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Add a column to `voter_df` named `random_val` with the results of the `F.rand()` method for any voter with the title Councilmember. Set `random_val` to 2 for the Mayor. Set any other title to the value 0.\n",
    "- Show some of the Data Frame rows, noting whether the clauses worked.\n",
    "- Use the `.filter clause` to find 0 in `random_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3641e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.12199359756154504|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston| 0.5075754959274408|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano| 0.5107135594922975|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|0.25281531965598836|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.6367966569760133|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs| 0.6208505837706111|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough| 0.3755299590057183|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman| 0.0043807137205083|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|0.10434890526842433|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.49729072649111006|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston| 0.4549509058868576|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|0.42422442012922856|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|0.44902202956892057|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.6927845079677668|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|  0.720005954910272|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.0781563376240837|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson| 0.4143635432704451|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.6462758929863991|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+-----------------+--------------------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|              splits|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+--------------------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|       Mayor Pro Tem|    Casey  Thomas|     [Casey, Thomas]|     Casey|   Thomas|       0.0|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|06/13/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|06/13/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+--------------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa0412",
   "metadata": {},
   "source": [
    "## User defined functions\n",
    "1. User defined functions\n",
    "\n",
    "We've looked at the built-in functions in Spark and have had great results using these. But let's consider what you would do if you needed to apply some custom logic to your data cleaning processes.\n",
    "\n",
    "2. Defined...\n",
    "\n",
    "A user defined function, or UDF, is a Python method that the user writes to perform a specific bit of logic. Once written, the method is called via the pyspark.sql.functions.udf() method. The result is stored as a variable and can be called as a normal Spark function. Let's look at a couple examples.\n",
    "\n",
    "3. Reverse string UDF\n",
    "\n",
    "Here is a fairly trivial example to illustrate how a UDF is defined. First, we define a python function. We'll call our function, reverseString(), with an argument called mystr. We'll use some python shorthand to reverse the string and return it. Don't worry about understanding how the return statement works, only that it will reverse the lettering of whatever is fed into it (ie, \"help\" becomes \"pleh\"). The next step is to wrap the function and store it in a variable for later use. We'll use the pyspark.sql.functions.udf() method. It takes two arguments - the name of the method you just defined, and the Spark data type it will return. This can be any of the options in pyspark.sql.types, and can even be a more complex type, including a fully defined schema object. Most often, you'll return either a simple object type, or perhaps an ArrayType. We'll call udf with our new method name, and use the StringType(), then store this as udfReverseString(). Finally, we use our new UDF to add a column to the user_df DataFrame within the .withColumn() method. Note that we pass the column we're interested in as the argument to udfReverseString(). The udf function is called for each row of the Data Frame. Under the hood, the udf function takes the value stored for the specified column (per row) and passes it to the python method. The result is fed back to the resulting DataFrame.\n",
    "\n",
    "4. Argument-less example\n",
    "\n",
    "Another quick example is using a function that does not require an argument. We're defining our sortingCap() function to return one of the letters 'G', 'H', 'R', or 'S' at random. We still create our udf wrapped function, and define the return type as StringType(). The primary difference is calling the function, this time without passing in an argument as it is not required.\n",
    "\n",
    "5. Let's practice!\n",
    "\n",
    "As always, the best way to learn is practice - let's create some user defined functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526f9aa",
   "metadata": {},
   "source": [
    "### Understanding user defined functions\n",
    "When creating a new user defined function, which is not a possible value for the second argument?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "- ArrayType(IntegerType())\n",
    "\n",
    "- IntegerType()\n",
    "\n",
    "- LongType()\n",
    "\n",
    "- **udf()**\n",
    "\n",
    "- StringType()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1013aa",
   "metadata": {},
   "source": [
    "### Using user defined functions in Spark\n",
    "You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "For this exercise, we'll use our voter_df DataFrame, but you're going to replace the `first_name` column with the first and middle names.\n",
    "\n",
    "The `pyspark.sql.functions` library is available under the alias `F`. The classes from `pyspark.sql.types` are already imported.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Edit the `getFirstAndMiddle()` function to return a space separated string of names, except the last entry in the names list.\n",
    "- Define the function as a user-defined function. It should return a string type.\n",
    "- Create a new column on `voter_df` called `first_and_middle_name` using your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e77f6de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|         random_val|first_and_middle_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.12199359756154504|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston| 0.5075754959274408|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano| 0.5107135594922975|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|0.25281531965598836|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.6367966569760133|         Carolyn King|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs| 0.6208505837706111|                Scott|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough| 0.3755299590057183|              B. Adam|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman| 0.0043807137205083|                  Lee|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|0.10434890526842433|                Sandy|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.49729072649111006|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston| 0.4549509058868576|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|0.42422442012922856|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|0.44902202956892057|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.6927845079677668|         Carolyn King|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|  0.720005954910272|            Rickey D.|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.0781563376240837|          Jennifer S.|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson| 0.4143635432704451|                Sandy|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.6462758929863991|          Jennifer S.|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b63c0",
   "metadata": {},
   "source": [
    "## Partitioning and lazy processing\n",
    "1. Partitioning and lazy processing\n",
    "\n",
    "Welcome back to our discussion about modifying and cleaning DataFrames. We've discussed various transformations and methods to modify our data, but we haven't covered much about how Spark actually processes the data. Let's look at that now.\n",
    "\n",
    "2. Partitioning\n",
    "\n",
    "Spark breaks DataFrames into partitions, or chunks of data. These partitions can be automatically defined, enlarged, shrunk, and can differ greatly based on the type of Spark cluster being used. The size of the partition does vary, but generally try to keep your partition sizes equal. We'll discuss more about optimizing partitioning and cluster details later on. For now, let's assume that each partition is handled independently. This is part of what provides the performance levels and horizontal scaling ability in Spark. If a Spark node doesn't need to compete for resources, nor consult with other Spark nodes for answers, it can reliably schedule the processing for the best performance.\n",
    "\n",
    "3. Lazy processing\n",
    "\n",
    "In Spark, any transformation operation is lazy; it's more like a recipe than a command. It defines what should be done to a DataFrame rather than actually doing it. Most operations in Spark are actually transformations, including .withColumn(), .select(), .filter(), and so forth. The set of transformations you define are only executed when you run a Spark action. This includes .count(), .write(), etc - anything that requires the transformations to be run to properly obtain an answer. Spark can reorder transformations for the best performance. Usually this isn't noticeable, but can occasionally cause unexpected behavior, such as IDs not being added until after other transformations have completed. This doesn't actually cause a problem but the data can look unusual if you don't know what to expect.\n",
    "\n",
    "4. Adding IDs\n",
    "\n",
    "Relational databases tend to have a field used to identify the row, whether it is for an actual relationship reference, or just for data identification. These IDs are typically an integer that increases in value, is sequential, and most importantly unique. The problem with these IDs is they're not very parallel in nature. Given that the values are given out sequentially, if there are multiple workers, they must all refer to a common source for the next entry. This is OK in a single server environment, but in a distributed platform such as Spark, it creates some undue bottlenecks. Let's take a look at how to generate ID's in Spark.\n",
    "\n",
    "5. Monotonically increasing IDs\n",
    "\n",
    "Spark has a built-in function called monotonically_increasing_id(), designed to provide an integer ID that increases in value and is unique. These IDs are not necessarily sequential - there can be gaps, often quite large, between values. Unlike a normal relational ID, Spark's is completely parallel - each partition is allocated up to 8 billion IDs that can be assigned. Notice that the ID fields in the sample table are integers, increasing in value, but are not sequential. It's a little out scope, but the IDs are a 64-bit number effectively split into groups based on the Spark partition. Each group contains 8.4 billion IDs, and there are 2.1 billion possible groups, none of which overlap.\n",
    "\n",
    "6. Notes\n",
    "\n",
    "There's a lot of nuance to how partitions and the monotonically increasing ID's work. Remembering that Spark is lazy: it often helps in troubleshooting what can happen. Operations are often out of order - especially if joins are involved. It's best to test your transformations.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "We've discussed a lot of detail in this lesson - let's now take a look at a few exercises that will help solidify how everything works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b71c5",
   "metadata": {},
   "source": [
    "### Adding an ID Field\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset.\n",
    "\n",
    "The spark session and a Spark DataFrame `df` containing the `DallasCouncilVotes.csv.gz` file are available in your workspace. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Select the unique entries from the column `VOTER NAME` and create a new DataFrame called `voter_df`.\n",
    "- Count the rows in the `voter_df` DataFrame.\n",
    "- Add a `ROW_ID` column using the appropriate Spark function.\n",
    "- Show the rows with the 10 highest `ROW_IDs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc8e828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('DallasCouncilVoters.csv.gz',header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10047aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 36 rows in the voter_df DataFrame.\n",
      "\n",
      "+--------------------+------+\n",
      "|          VOTER_NAME|ROW_ID|\n",
      "+--------------------+------+\n",
      "|        Lee Kleinman|    35|\n",
      "|  the  final  201...|    34|\n",
      "|         Erik Wilson|    33|\n",
      "|  the  final   20...|    32|\n",
      "| Carolyn King Arnold|    31|\n",
      "| Rickey D.  Callahan|    30|\n",
      "|   the   final  2...|    29|\n",
      "|    Monica R. Alonzo|    28|\n",
      "|     Lee M. Kleinman|    27|\n",
      "|   Jennifer S. Gates|    26|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER_NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a1fbd",
   "metadata": {},
   "source": [
    "## IDs with different partitions\n",
    "You've just completed adding an ID field to a DataFrame. Now, take a look at what happens when you do the same thing on DataFrames containing a different number of partitions.\n",
    "\n",
    "To check the number of partitions, use the method `.rdd.getNumPartitions()` on a DataFrame.\n",
    "\n",
    "The spark session and two DataFrames, `voter_df` and `voter_df_single`, are available in your workspace. The instructions will help you discover the difference between the DataFrames. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Print the number of partitions on each DataFrame.\n",
    "- Add a `ROW_ID` field to each DataFrame.\n",
    "- Show the top 10 IDs in each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf7ae46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df_single = df\n",
    "voter_df = df\n",
    "voter_df = voter_df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7fd338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 5 partitions in the voter_df DataFrame.\n",
      "\n",
      "\n",
      "There are 1 partitions in the voter_df_single DataFrame.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------------+-----------+\n",
      "|      DATE|        TITLE|       VOTER_NAME|     ROW_ID|\n",
      "+----------+-------------+-----------------+-----------+\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747292|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747291|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747290|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747289|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747288|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747287|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747286|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747285|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747284|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747283|\n",
      "+----------+-------------+-----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+--------------------+-------------------+------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|ROW_ID|\n",
      "+----------+--------------------+-------------------+------+\n",
      "|11/20/2018|       Councilmember|      Mark  Clayton| 44624|\n",
      "|11/20/2018|       Councilmember|     Tennell Atkins| 44623|\n",
      "|11/20/2018|       Councilmember|       Kevin Felder| 44622|\n",
      "|11/20/2018|       Councilmember|       Omar Narvaez| 44621|\n",
      "|11/20/2018|       Councilmember|Rickey D.  Callahan| 44620|\n",
      "|11/20/2018|              Vacant|               null| 44619|\n",
      "|11/20/2018|       Mayor Pro Tem|      Casey  Thomas| 44618|\n",
      "|11/20/2018|Deputy Mayor Pro Tem|       Adam Medrano| 44617|\n",
      "|11/20/2018|               Mayor|Michael S. Rawlings| 44616|\n",
      "|11/20/2018|       Councilmember|Philip T.  Kingston| 44615|\n",
      "+----------+--------------------+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461011d9",
   "metadata": {},
   "source": [
    "## More ID tricks\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month.\n",
    "\n",
    "The spark session and two DataFrames, `voter_df_march` and `voter_df_april`, are available in your workspace. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Determine the highest `ROW_ID` in `voter_df_march` and save it in the variable `previous_max_ID`. The statement `.rdd.max()[0]` will get the maximum ID.\n",
    "- Add a `ROW_ID` column to `voter_df_april` starting at the value of `previous_max_ID`.\n",
    "- Show the `ROW_ID`'s from both Data Frames and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81898d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|\n",
      "+----------+--------------------+-------------------+\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|\n",
      "|03/21/2018|       Councilmember|Philip T.  Kingston|\n",
      "|03/21/2018|               Mayor|Michael S. Rawlings|\n",
      "|03/21/2018|Deputy Mayor Pro Tem|       Adam Medrano|\n",
      "|03/21/2018|       Councilmember|       Casey Thomas|\n",
      "|03/21/2018|       Mayor Pro Tem|  Dwaine R. Caraway|\n",
      "|03/21/2018|       Councilmember|Rickey D.  Callahan|\n",
      "|03/21/2018|       Councilmember|       Omar Narvaez|\n",
      "|03/21/2018|       Councilmember|       Kevin Felder|\n",
      "|03/21/2018|       Councilmember|     Tennell Atkins|\n",
      "|03/21/2018|       Councilmember|      Mark  Clayton|\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df_march = df.filter(\"(DATE > '02/28/2018') and (DATE < '04/01/2018')\")\n",
    "voter_df_march.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e625439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|\n",
      "+----------+--------------------+-------------------+\n",
      "|04/25/2018|       Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|       Councilmember| Jennifer S.  Gates|\n",
      "|04/25/2018|       Councilmember|Philip T.  Kingston|\n",
      "|04/25/2018|               Mayor|Michael S. Rawlings|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|       Adam Medrano|\n",
      "|04/25/2018|       Councilmember|       Casey Thomas|\n",
      "|04/25/2018|       Mayor Pro Tem|  Dwaine R. Caraway|\n",
      "|04/25/2018|       Councilmember|Rickey D.  Callahan|\n",
      "|04/25/2018|       Councilmember|       Omar Narvaez|\n",
      "|04/25/2018|       Councilmember|       Kevin Felder|\n",
      "|04/25/2018|       Councilmember|     Tennell Atkins|\n",
      "|04/25/2018|       Councilmember|      Mark  Clayton|\n",
      "|04/25/2018|       Councilmember|      Scott  Griggs|\n",
      "|04/25/2018|       Councilmember|   B. Adam  McGough|\n",
      "|04/25/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|04/25/2018|       Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|       Councilmember| Jennifer S.  Gates|\n",
      "|04/25/2018|       Councilmember|   B. Adam  McGough|\n",
      "|04/25/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|04/25/2018|       Councilmember|     Sandy  Greyson|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df_april = df.filter(\"(DATE > '03/31/2018') and (DATE < '05/01/2018')\")\n",
    "voter_df_april.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a80a5422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|ROW_ID|\n",
      "+----------+--------------------+-------------------+------+\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|     0|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|     1|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|     2|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|     3|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|     4|\n",
      "|03/21/2018|       Councilmember|Philip T.  Kingston|     5|\n",
      "|03/21/2018|               Mayor|Michael S. Rawlings|     6|\n",
      "|03/21/2018|Deputy Mayor Pro Tem|       Adam Medrano|     7|\n",
      "|03/21/2018|       Councilmember|       Casey Thomas|     8|\n",
      "|03/21/2018|       Mayor Pro Tem|  Dwaine R. Caraway|     9|\n",
      "|03/21/2018|       Councilmember|Rickey D.  Callahan|    10|\n",
      "|03/21/2018|       Councilmember|       Omar Narvaez|    11|\n",
      "|03/21/2018|       Councilmember|       Kevin Felder|    12|\n",
      "|03/21/2018|       Councilmember|     Tennell Atkins|    13|\n",
      "|03/21/2018|       Councilmember|      Mark  Clayton|    14|\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|    15|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|    16|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|    17|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|    18|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|    19|\n",
      "+----------+--------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df_march = voter_df_march.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_march.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "620a5fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|  1109|\n",
      "|  1110|\n",
      "|  1111|\n",
      "|  1112|\n",
      "|  1113|\n",
      "|  1114|\n",
      "|  1115|\n",
      "|  1116|\n",
      "|  1117|\n",
      "|  1118|\n",
      "|  1119|\n",
      "|  1120|\n",
      "|  1121|\n",
      "|  1122|\n",
      "|  1123|\n",
      "|  1124|\n",
      "|  1125|\n",
      "|  1126|\n",
      "|  1127|\n",
      "|  1128|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108f0a7",
   "metadata": {},
   "source": [
    "# Chapter 3: Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab518e2",
   "metadata": {},
   "source": [
    "## Caching\n",
    "1. Caching\n",
    "\n",
    "Now that we've done some data cleaning tasks using Spark, let's look at how to improve the performance of running those tasks using caching.\n",
    "\n",
    "2. What is caching?\n",
    "\n",
    "Caching in Spark refers to storing the results of a DataFrame in memory or on disk of the processing nodes in a cluster. Caching improves the speed for subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source. Using caching reduces the resource utilization of the cluster - there is less need to access the storage, networking, and CPU of the Spark nodes as the data is likely already present.\n",
    "\n",
    "3. Disadvantages of caching\n",
    "\n",
    "There are a few disadvantages of caching you should be aware of. Very large data sets may not fit in the memory reserved for cached DataFrames. Depending on the later transformations requested, the cache may not do anything to help performance. If a data set does not stay cached in memory, it may be persisted to disk. Depending on the disk configuration of a Spark cluster, this may not be a large performance improvement. If you're reading from a local network resource and have slow local disk I/O, it may be better to avoid caching the objects. Finally, the lifetime of a cached object is not guaranteed. Spark handles regenerating DataFrames for you automatically, but this can cause delays in processing.\n",
    "\n",
    "4. Caching tips\n",
    "\n",
    "Caching is incredibly useful, but only if you plan to use the DataFrame again. If you only need it for a single task, it's not worth caching. The best way to gauge performance with caching is to test various configurations. Try caching your DataFrames at various points in the processing cycle and check if it improves your processing time. Try to cache in memory or fast NVMe / SSD storage. While still slower than main memory modern SSD based storage is drastically faster than spinning disk. Local spinning hard drives can still be useful if you are processing large DataFrames that require a lot of steps to generate, or must be accessed over the Internet. Testing this is crucial. If normal caching doesn't seem to work, try creating intermediate Parquet representations like we did in Chapter 1. These can provide a checkpoint in case a job fails mid-task and can still be used with caching to further improve performance. Finally, you can manually stop caching a DataFrame when you're finished with it. This frees up cache resources for other DataFrames.\n",
    "\n",
    "5. Implementing caching\n",
    "\n",
    "Implementing caching in Spark is simple. The primary way is to call the function .cache() on a DataFrame object prior to a given Action. It requires no arguments. One example is creating a DataFrame from some original CSV data. Prior to running a .count() on the data, we call .cache() to tell Spark to store it in cache. Another option is to call .cache() separately. Here we create an ID in one transformation. We then call .cache() on the DataFrame. When we call the .show() action, the voter_df DataFrame will be cached. If you're following closely, this means that .cache() is a Spark transformation - nothing is actually cached until an action is called.\n",
    "\n",
    "6. More cache operations\n",
    "\n",
    "A couple other options are available with caching in Spark. To check if a DataFrame is cached, use the .is_cached boolean property which returns True (as in this case) or False. To un-cache a DataFrame, we call .unpersist() with no arguments. This removes the object from the cache.\n",
    "\n",
    "7. Let's Practice!\n",
    "\n",
    "We've discussed caching in depth - let's practice how to use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583412a",
   "metadata": {},
   "source": [
    "### Caching a DataFrame\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "The DataFrame `departures_df` is defined, but no actions have been performed.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Cache the unique rows in the `departures_df` DataFrame.\n",
    "- Perform a count query on `departures_df`, noting how long the operation takes.\n",
    "- Count the rows again, noting the variance in time of a cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5794e64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607512"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df14 = spark.read.csv('AA_DFW_2014_Departures_Short.csv.gz', header= True)\n",
    "df15 = spark.read.csv('AA_DFW_2015_Departures_Short.csv.gz', header= True)\n",
    "df16 = spark.read.csv('AA_DFW_2014_Departures_Short.csv.gz', header= True)\n",
    "df17 = spark.read.csv('AA_DFW_2015_Departures_Short.csv.gz', header= True)\n",
    "departures_df = df14.union(df15).union(df16).union(df17)\n",
    "departures_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adb55703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ebbfef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 303756 rows took 7.531385 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:============================================>         (163 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 303756 rows again took 1.971110 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 72:==================================================>   (188 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802dd16d",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache\n",
    "You've finished the analysis tasks with the `departures_df` DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
    "\n",
    "The DataFrame `departures_df` is defined and has already been cached for you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Check the caching status on the `departures_df` DataFrame.\n",
    "- Remove the `departures_df` DataFrame from the cache.\n",
    "- Validate the caching status again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae99804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a3e1c",
   "metadata": {},
   "source": [
    "## Improve import performance\n",
    "1. Improve import performance\n",
    "\n",
    "We've discussed the benefits of caching when working with Spark DataFrames. Let's look at how to improve the speed when getting data into a DataFrame.\n",
    "\n",
    "2. Spark clusters\n",
    "\n",
    "Spark clusters consist of two types of processes - one driver process and as many worker processes as required. The driver handles task assignments and consolidation of the data results from the workers. The workers typically handle the actual transformation / action tasks of a Spark job. Once assigned tasks, they operate fairly independently and report results back to the driver. It is possible to have a single node Spark cluster (this is what we're using for this course) but you'll rarely see this in a production environment. There are different ways to run Spark clusters - the method used depends on your specific environment.\n",
    "\n",
    "3. Import performance\n",
    "\n",
    "When importing data to Spark DataFrames, it's important to understand how the cluster implements the job. The process varies depending on the type of task, but it's safe to assume that the more import objects available, the better the cluster can divvy up the job. This may not matter on a single node cluster, but with a larger cluster each worker can take part in the import process. In clearer terms, one large file will perform considerably worse than many smaller ones. Depending on the configuration of your cluster, you may not be able to process larger files, but could easily handle the same amount of data split between smaller files. Note you can define a single import statement, even if there are multiple files. You can use any form of standard wildcard symbol when defining the import filename. While less important, if objects are about the same size, the cluster will perform better than having a mix of very large and very small objects.\n",
    "\n",
    "4. Schemas\n",
    "\n",
    "If you remember from chapter one, we discussed the importance of Spark schemas. Well-defined schemas in Spark drastically improve import performance. Without a schema defined, import tasks require reading the data multiple times to infer structure. This is very slow when you have a lot of data. Spark may not define the objects in the data the same as you would. Spark schemas also provide validation on import. This can save steps with data cleaning jobs and improve the overall processing time.\n",
    "\n",
    "5. How to split objects\n",
    "\n",
    "There are various effective ways to split an object (files mostly) into more smaller objects. The first is to use built-in OS utilities such as split, cut, or awk. An example using split uses the -l argument with the number of lines to have per file (10000 in this case). The -d argument tells split to use numeric suffixes. The last two arguments are the name of the file to be split and the prefix to be used. Assuming 'largefile' has 10M records, we would have files named chunk-0000 through chunk-9999. Another method is to use python (or any other language) to split the objects up as we see fit. Sometimes you may not have the tools available to split a large file. If you're going to be working with a DataFrame often, a simple method is to read in the single file then write it back out as parquet. We've done this in previous examples and it works well for later analysis even if the initial import is slow. It's important to note that if you're hitting limitations due to cluster sizing, try to do as little processing as possible before writing to parquet.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "Let's practice some of the import tricks we've discussed now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82684d",
   "metadata": {},
   "source": [
    "### File size optimization\n",
    "Consider if you're given 2 large data files on a cluster with 10 nodes. Each file contains 10M rows of roughly the same size. While working with your data, the responsiveness is acceptable but the initial read from the files takes a considerable period of time. Note that you are the only one who will use the data and it changes for each run.\n",
    "\n",
    "Which of the following is the best option to improve performance?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "- Split the 2 files into 8 files of 2.5M rows each.\n",
    "\n",
    "- Convert the files to parquet.\n",
    "\n",
    "- **Split the 2 files into 50 files of 400K rows each.**\n",
    "\n",
    "- Split the files into 30 files containing a random number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5db12",
   "metadata": {},
   "source": [
    "### File import performance\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: `departures_full.txt.gz` and `departures_xxx.txt.gz` where xxx is 000 - 013. The same number of rows is split between each file.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `departures_full.txt.gz` file and the `departures_xxx.txt.gz` files into separate DataFrames.\n",
    "- Run a count on each DataFrame and compare the run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "688e4e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write one file with Pandas DF\n",
    "departures_df.toPandas().to_csv('departures_full.txt', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce235a",
   "metadata": {},
   "source": [
    "- I Used `split -l 10000 -d departures_full.txt dep/departures_` to create chunk files in the `dep`folder\n",
    "- To run a shell command in Jupyter, I used the magic keyword `%%bash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aff04b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "split -l 10000 -d departures_full.txt dep/departures_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a9e1511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t303757\n",
      "Time to run: 0.254781\n",
      "Total rows in split DataFrame:\t303757\n",
      "Time to run: 0.242451\n"
     ]
    }
   ],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt')\n",
    "split_df = spark.read.csv('dep/departures_*')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217d58c",
   "metadata": {},
   "source": [
    "## Cluster configurations\n",
    "1. Cluster sizing tips\n",
    "\n",
    "We've just finished working with improving import performance in Spark. Let's take a look at cluster configurations.\n",
    "\n",
    "2. Configuration options\n",
    "\n",
    "Spark has many available configuration settings controlling all aspects of the installation. These configurations can be modified to best match the specific needs for the cluster. The configurations are available in the configuration files, via the Spark web interface, and via the run-time code. Our test cluster is only accessible via command shell so we'll use the last option. To read a configuration setting, call spark.conf.get() with the name of the setting as the argument. To write a configuration setting, call spark.conf.set() with the name of the setting and the actual value as the function arguments.\n",
    "\n",
    "3. Cluster Types\n",
    "\n",
    "Spark deployments can vary depending on the exact needs of the users. One large component of a deployment is the cluster management mechanism. Spark clusters can be: Single node clusters, deploying all components on a single system (physical / VM / container). Standalone clusters, with dedicated machines as the driver and workers. Managed clusters, meaning that the cluster components are handled by a third party cluster manager such as YARN, Mesos, or Kubernetes. In this course, we're using a single node cluster. Your production environment can vary wildly, but we'll discuss standalone clusters as the concepts flow across all management types.\n",
    "\n",
    "4. Driver\n",
    "\n",
    "If you recall, there is one driver per Spark cluster. The driver is responsible for several things, including the following: Handling task assignment to the various nodes / processes in the cluster. The driver monitors the state of all processes and tasks and handles any task retries. The driver is also responsible for consolidating results from the other processes in the cluster. The driver handles any access to shared data and verifies each worker process has the necessary resources (code, data, etc). Given the importance of the driver, it is often worth increasing the specifications of the node compared to other systems. Doubling the memory compared to other nodes is recommended. This is useful for task monitoring and data consolidation tasks. As with all Spark systems, fast local storage is useful for running Spark in an ideal setup.\n",
    "\n",
    "5. Worker\n",
    "\n",
    "A Spark worker handles running tasks assigned by the driver and communicates those results back to the driver. Ideally, the worker has a copy of all code, data, and access to the necessary resources required to complete a given task. If any of these are unavailable, the worker must pause to obtain the resources. When sizing a cluster, there are a few recommendations: Depending on the type of task, more worker nodes is often better than larger nodes. This can be especially obvious during import and export operations as there are more machines available to do the work. As with everything in Spark, test various configurations to find the correct balance for your workload. Assuming a cloud environment, 16 worker nodes may complete a job in an hour and cost $50 in resources. An 8 worker configuration might take 1.25 hrs but cost only half as much. Finally, workers can make use of fast local storage (SSD / NVMe) for caching, intermediate files, etc.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "Now that we've discussed cluster sizing and configuration, let's practice working with these options!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd782198",
   "metadata": {},
   "source": [
    "### Reading Spark configurations\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
    "\n",
    "The spark object is available for use.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Check the name of the Spark application instance (`'spark.app.name'`).\n",
    "- Determine the TCP port the driver runs on (`'spark.driver.port'`).\n",
    "- Determine how many partitions are configured for joins.\n",
    "- Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64f7500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 35959\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ba658",
   "metadata": {},
   "source": [
    "### Writing Spark configurations\n",
    "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster.\n",
    "\n",
    "The spark configuration is initially set to the default value of 200 partitions.\n",
    "\n",
    "The spark object is available for use. A file named `departures.txt.gz` is available for import. An initial DataFrame containing the distinct rows from `departures.txt.gz` is available as `departures_df`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Store the number of partitions in `departures_df` in the variable before.\n",
    "- Change the `spark.sql.shuffle.partitions` configuration to 500 partitions.\n",
    "- Recreate the `departures_df` DataFrame reading the distinct rows from the departures file.\n",
    "- Print the number of partitions from before and after the configuration change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3cb0a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 88:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count after change: 4\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('departures_full.txt').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5a37b",
   "metadata": {},
   "source": [
    "## Performance improvements\n",
    "1. Performance improvements\n",
    "\n",
    "We've discussed Spark clusters and improving import performance. Let's look at how to improve the performance of Spark tasks in general.\n",
    "\n",
    "2. Explaining the Spark execution plan\n",
    "\n",
    "To understand performance implications of Spark, you must be able to see what it's doing under the hood. The easiest way to do this is to use the .explain() function on a DataFrame. This example is taken from an earlier exercise, simply requesting a single column and running distinct against it. The result is the estimated plan that will be run to generate results from the DataFrame. Don't worry about the specifics of the plan yet, just remember how to view it if needed.\n",
    "\n",
    "3. What is shuffling?\n",
    "\n",
    "Spark distributes data amongst the various nodes in the cluster. A side effect of this is what is known as shuffling. Shuffling is the moving of data fragments to various workers as required to complete certain tasks. Shuffling is useful and hides overall complexity from the user (the user doesn't have to know which nodes have what data). That being said, it can be slow to complete the necessary transfers, especially if a few nodes require all the data. Shuffling lowers the overall throughput of the cluster as the workers must spend time waiting for the data to transfer. This limits the amount of available workers for the remaining tasks in the system. Shuffling is often a necessary component, but it's helpful to try to minimize it as much as possible.\n",
    "\n",
    "4. How to limit shuffling?\n",
    "\n",
    "It can be tricky to remove shuffling operations entirely but there are a few things that can limit it. The DataFrame .repartition() function takes a single argument, the number of partitions requested. We've used this in an earlier chapter to illustrate the effect of partitions with the monotonically_increasing_id() function. Repartitioning requires a full shuffle of data between nodes & processes and is quite costly. If you need to reduce the number of partitions, use the .coalesce() function instead. It takes a number of partitions smaller than the current one and consolidates the data without requiring a full data shuffle. Note: calling .coalesce() with a larger number of partitions does not actually do anything. The .join() function is a great use of Spark and provides a lot of power. Calling .join() indiscriminately can often cause shuffle operations, leading to increased cluster load & slower processing times. To avoid some of the shuffle operations when joining Spark DataFrames you can use the .broadcast() function. We'll talk about this more in a moment. Finally, an important note about data cleaning operations is remembering to optimize for what matters. The speed of your initial code may be perfectly acceptable and time may be better spent elsewhere.\n",
    "\n",
    "5. Broadcasting\n",
    "\n",
    "Broadcasting in Spark is a method to provide a copy of an object to each worker. When each worker has its own copy of the data, there is less need for communication between nodes. This limits data shuffles and it's more likely a node will fulfill tasks independently. Using broadcasting can drastically speed up .join() operations, especially if one of the DataFrames being joined is much smaller than the other. To implement broadcasting, you must import the broadcast function from pyspark.sql.functions. Once imported, simply call the broadcast function with the name of the DataFrame you wish to broadcast. Note broadcasting can slow operations when using very small DataFrames or if you broadcast the larger DataFrame in a join. Spark will often optimize this for you, but as usual, run tests in your environment for best performance.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "We've looked how to limit shuffling and implement broadcasting for DataFrames. Let's practice utilizing these tools now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902596f2",
   "metadata": {},
   "source": [
    "### Normal joins\n",
    "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan.\n",
    "\n",
    "The DataFrame `flights_df` and `airports_df` are available to you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a new DataFrame `normal_df` by joining `flights_df` with `airports_df`.\n",
    "- Determine which type of join is used in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14e7c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "airports_df = spark.read.csv('airports.csv', header= True, inferSchema=True).withColumnRenamed('dst','IATA')\n",
    "flights_df = spark.read.csv('flights_small.csv', header= True, inferSchema=True).withColumnRenamed('Dest','Destination Airport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b56dd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#1278], [IATA#1220], Inner, BuildRight, false\n",
      "   :- Project [year#1246, month#1247, day#1248, dep_time#1249, dep_delay#1250, arr_time#1251, arr_delay#1252, carrier#1253, tailnum#1254, flight#1255, origin#1256, dest#1257 AS Destination Airport#1278, air_time#1258, distance#1259, hour#1260, minute#1261]\n",
      "   :  +- Filter isnotnull(dest#1257)\n",
      "   :     +- FileScan csv [year#1246,month#1247,day#1248,dep_time#1249,dep_delay#1250,arr_time#1251,arr_delay#1252,carrier#1253,tailnum#1254,flight#1255,origin#1256,dest#1257,air_time#1258,distance#1259,hour#1260,minute#1261] Batched: false, DataFilters: [isnotnull(dest#1257)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/mnt/My_Drive/my_drive/DataCamp/pyspark/03_Cleaning_Data_with_PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:int,month:int,day:int,dep_time:string,dep_delay:string,arr_time:string,arr_delay:stri...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[6, string, true]),false), [plan_id=1346]\n",
      "      +- Project [faa#1206, name#1207, lat#1208, lon#1209, alt#1210, tz#1211, dst#1212 AS IATA#1220]\n",
      "         +- Filter isnotnull(dst#1212)\n",
      "            +- FileScan csv [faa#1206,name#1207,lat#1208,lon#1209,alt#1210,tz#1211,dst#1212] Batched: false, DataFilters: [isnotnull(dst#1212)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/mnt/My_Drive/my_drive/DataCamp/pyspark/03_Cleaning_Data_with_PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(dst)], ReadSchema: struct<faa:string,name:string,lat:double,lon:double,alt:int,tz:int,dst:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "                            \n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e452c35",
   "metadata": {},
   "source": [
    "### Using broadcasting on Spark joins\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "\n",
    "- Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "- On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "- If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.\n",
    "\n",
    "The DataFrames `flights_df` and `airports_df` are available to you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `broadcast()` method from `pyspark.sql.functions`.\n",
    "- Create a new DataFrame `broadcast_df` by joining `flights_df` with `airports_df`, using the broadcasting.\n",
    "- Show the query plan and consider differences from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ef127eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#1278], [IATA#1220], Inner, BuildRight, false\n",
      "   :- Project [year#1246, month#1247, day#1248, dep_time#1249, dep_delay#1250, arr_time#1251, arr_delay#1252, carrier#1253, tailnum#1254, flight#1255, origin#1256, dest#1257 AS Destination Airport#1278, air_time#1258, distance#1259, hour#1260, minute#1261]\n",
      "   :  +- Filter isnotnull(dest#1257)\n",
      "   :     +- FileScan csv [year#1246,month#1247,day#1248,dep_time#1249,dep_delay#1250,arr_time#1251,arr_delay#1252,carrier#1253,tailnum#1254,flight#1255,origin#1256,dest#1257,air_time#1258,distance#1259,hour#1260,minute#1261] Batched: false, DataFilters: [isnotnull(dest#1257)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/mnt/My_Drive/my_drive/DataCamp/pyspark/03_Cleaning_Data_with_PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:int,month:int,day:int,dep_time:string,dep_delay:string,arr_time:string,arr_delay:stri...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[6, string, true]),false), [plan_id=1368]\n",
      "      +- Project [faa#1206, name#1207, lat#1208, lon#1209, alt#1210, tz#1211, dst#1212 AS IATA#1220]\n",
      "         +- Filter isnotnull(dst#1212)\n",
      "            +- FileScan csv [faa#1206,name#1207,lat#1208,lon#1209,alt#1210,tz#1211,dst#1212] Batched: false, DataFilters: [isnotnull(dst#1212)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/mnt/My_Drive/my_drive/DataCamp/pyspark/03_Cleaning_Data_with_PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(dst)], ReadSchema: struct<faa:string,name:string,lat:double,lon:double,alt:int,tz:int,dst:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f578d",
   "metadata": {},
   "source": [
    "### Comparing broadcast vs normal joins\n",
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed.\n",
    "\n",
    "Your DataFrames `normal_df` and `broadcast_df` are available for your use.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Execute `.count()` on the normal DataFrame.\n",
    "- Execute `.count()` on the broadcasted DataFrame.\n",
    "- Print the count and duration of the DataFrames noting and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6df83ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t0\tduration: 0.337095\n",
      "Broadcast count:\t0\tduration: 0.305110\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f55b4",
   "metadata": {},
   "source": [
    "# Complex processing and data pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ccfb26",
   "metadata": {},
   "source": [
    "## Introduction to data pipelines\n",
    "1. Introduction to Data Pipelines\n",
    "\n",
    "We've spent most of this course working with individual transformations and actions to clean data in Spark. But data is rarely so simple that a couple transformations or actions can prepare it for real analysis. Let's look now at data pipelines.\n",
    "\n",
    "2. What is a data pipeline?\n",
    "\n",
    "Data pipelines are simply the set of steps needed to move from an input data source, or sources, and convert it to the desired output. A data pipeline can consist of any number of steps or components, and can span many systems. For our purposes, we’ll be setting up a data pipeline within Spark, but realize that a full production data pipeline will likely communicate with many systems.\n",
    "\n",
    "3. What does a data pipeline look like?\n",
    "\n",
    "Much like Spark in general, a data pipeline typically consists of inputs, transformations, and the outputs of those steps. In addition, there is often validation and analysis steps before delivery of the data to the next user. An input can be any of the data types we've looked at so far, including CSV, JSON, text, etc. It could be from the local file system, or from web services, APIs, databases, and so on. The basic idea is to read the data into a DataFrame as we've done previously. Once we have the data in a DataFrame, we need to transform it in some fashion. You’ve done the individual steps several times throughout this course - adding columns, filtering rows, performing calculations as needed. In our previous examples, we’ve only done one or two of these steps at a time but a pipeline can consist of as many of these steps as needed so we can format the data into our desired output. After we’ve defined our transformations we need to output the data into a usable form. You’ve already written files out to CSV or Parquet format, but it could include multiple copies with various formats, or instead write the output to a database, a web service, etc. The last two steps vary greatly depending on your needs. We'll discuss validation in a later lesson, but the idea is to run some form of testing on the data to verify it is as expected. Analysis is often the final step before handing the data off to the next user. This can include things such as row counts, specific calculations, or pretty much anything that makes it easier for the user to consume the dataset.\n",
    "\n",
    "4. Pipeline details\n",
    "\n",
    "It's important to note that in Spark, a data pipeline is not a formally defined object, but rather a concept. This is different than if you've used the Pipeline object in Spark.ML (If you haven't, you don't worry, it's not needed for this course). For our purposes, a Spark pipeline is all normal code required to complete a task. In this example, we're doing the various tasks required to define a schema, read a datafile, add an ID, then write out two separate data types. The task could be much more complex, but the concept is usually the same.\n",
    "\n",
    "5. Let's Practice!\n",
    "\n",
    "When you look at the components, a data pipeline in Spark is fairly simple, but can be very powerful. Let's start working on a more elaborate data pipeline now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba2654",
   "metadata": {},
   "source": [
    "### Quick pipeline\n",
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\n",
    "The `spark` context is defined, along with the `pyspark.sql.functions` library being aliased as `F` as is customary.\n",
    "\n",
    "Instructions\n",
    "- Import the file `2015-departures.csv.gz` to a DataFrame. Note the header is already defined.\n",
    "- Filter the DataFrame to contain only flights with a duration over 0 minutes. Use the index of the column, not the column name (remember to use `.printSchema()` to see the column names / order).\n",
    "- Add an ID column.\n",
    "- Write the file out as a JSON document named `output.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d943d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('AA_DFW_2015_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "departures_df.printSchema()\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.where('\"Actual elapsed time (Minutes)\" > 0')\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json(\"output.json\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021f001",
   "metadata": {},
   "source": [
    "### Pipeline data issue\n",
    "After creating your quick pipeline, you provide the json file to an analyst on your team. After loading the data and performing a couple exploratory tasks, the analyst tells you there's a problem in the dataset while trying to sort the duration data. She's not sure what the issue is beyond the sorting operation not working as expected.\n",
    "\n",
    "|Date  |        Flight Number  | Airport    | Duration  |  ID|\n",
    "|---|---|---|---|---|\n",
    "|09/30/2015  |  2287      |      ANC    |     409 |        107962|\n",
    "|12/28/2015 |   1408     |       OKC     |    41   |       141917|\n",
    "|08/11/2015 |   2287     |       ANC     |    410   |      87978|\n",
    "\n",
    "After analyzing the data, which command would fix the issue?\n",
    "\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- departures_df = departures_df.orderBy(departures_df.Airport)\n",
    "\n",
    "- **departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))**\n",
    "\n",
    "- departures_df = departures_df.orderBy(departures_df['Duration'])\n",
    "\n",
    "- departures_df = departures_df.select(departures_df['Duration']).cast(LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d28e0",
   "metadata": {},
   "source": [
    "## Data handling techniques\n",
    "\n",
    "1. Data handling techniques\n",
    "\n",
    "We've worked with many aspects of Spark when it comes to data cleaning operations. Let's look at how to use some of the methods we've learned to parse unconventional data.\n",
    "\n",
    "2. What are we trying to parse?\n",
    "\n",
    "When reading data into Spark, you're rarely given a fully uniform file. Often there is content that needs to be removed or reformatted. Some common issues include: Incorrect data, consisting of empty rows, commented lines, headers, or even rows that don't match the intended schema. Real world data often includes nested structures, including columns that use different delimiters. This could include the primary columns separated via a comma, but including some components separated via a semi-colon. Real data often won't fit into a tabular format, sometimes consisting of a differing number of columns per row. There are various ways to parse data in all of these situations. The way you choose will depend on your specific needs. We are focusing on CSV data for this course, but the general scenarios described apply to other formats as well.\n",
    "\n",
    "3. Stanford ImageNet annotations\n",
    "\n",
    "For this chapter we're going to use the Stanford ImageNet annotations which focus on finding and identifying dogs in various ImageNet images. The annotations provide a list of all identified dogs in an image, including when multiple dogs are in the same image. Other metadata is included, including the folder within the ImageNet dataset, the image dimensions, and the bounding box(es) of the dog(s) in the image. In the example rows, we have the folder names, the ImageNet image reference, width, and height. Then there is the image data for the type of dog (or dogs) in each image. Each breed \"column\" consists of the breed name and the bounding box in the image. The first row contains one Newfoundland, but notice that the 2nd row actually has two Bull Mastiffs identified and has an addition \"column\" defined.\n",
    "\n",
    "4. Removing blank lines, headers, and comments\n",
    "\n",
    "Spark's CSV parser can handle many common data issues via optional parameters. Blank lines are automatically removed (unless specifically instructed otherwise) when using the CSV parsing. Comments can be removed with an optional named argument, comment, and specifying the character that any comment line would be defined by. Note that this handles lines that begin with a specific comment. Parsing more complex comment usage requires more involved procedures. Header rows can be parsed via an optional parameter named header, and set to 'True' or 'False'. If no schema is defined, column names will be initially set as defined by the header. If a schema is defined, the row is not used as data, but the header names are otherwise ignored.\n",
    "\n",
    "5. Automatic column creation\n",
    "\n",
    "When importing CSV data into Spark, it will automatically create DataFrame columns if it can. It will split a row of text from the CSV on a defined separator argument named 'sep'. If sep is not defined, it will default to using a comma. The CSV parser will still succeed in parsing data if the separator character is not within the string. It will store the entire row in a column named _c0 by default. Using this trick allows parsing of nested or complex data. We'll look at this more later on.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "Let's practice working with this data and extending our data pipeline further!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874f3af",
   "metadata": {},
   "source": [
    "### Removing commented lines\n",
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "To start, you need to remove all commented rows in the dataset.\n",
    "\n",
    "The `spark` context, and the base CSV file (`annotations.csv.gz`) are available for you to work with. The col function is also available for use.\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Import the `annotations.csv.gz` file to a DataFrame and perform a row count. Specify a separator character of |.\n",
    "- Query the data for the number of rows beginning with #.\n",
    "- Import the file again to a new DataFrame, but specify the comment character in the options to remove any commented rows.\n",
    "- Count the new DataFrame and verify the difference is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84759a35",
   "metadata": {},
   "source": [
    "import webbrowser\n",
    "import os\n",
    "\n",
    "file_path = os.path.realpath('annotations.csv.gz')\n",
    "webbrowser.open('file:///' + file_path, new =2)  # open in new tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "014947c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full count: 32794\n",
      "Comment count: 1416\n",
      "Remaining count: 31378\n"
     ]
    }
   ],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.filter(F.col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep=\"|\", comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c0e6d",
   "metadata": {},
   "source": [
    "### Removing invalid rows\n",
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (\\t) characters.\n",
    "\n",
    "The DataFrame `annotations_df` is already available, with the commented rows removed. The `spark.sql.functions` library is available under the alias `F`. The initial number of rows available in the DataFrame is stored in the variable `initial_count`.\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Create a new variable `tmp_fields` using the annotations_df DataFrame column `'_c0'` splitting it on the tab character.\n",
    "- Create a new column in `annotations_df` named `'colcount'` representing the number of fields defined in the previous step.\n",
    "- Filter out any rows from `annotations_df` containing fewer than 5 fields.\n",
    "- Count the number of rows in the DataFrame and compare to the `initial_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3689aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count = no_comments_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8061e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count: 31378\n",
      "Final count: 20580\n"
     ]
    }
   ],
   "source": [
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "#print(tmp_fields.cols)\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (F.size(tmp_fields) < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cec627",
   "metadata": {},
   "source": [
    "### Splitting into columns\n",
    "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content.\n",
    "\n",
    "You have the `spark` context and the latest version of the `annotations_df` DataFrame. `pyspark.sql.functions` is available under the alias `F`.\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Split the content of the `'_c0'` column on the tab character and store in a variable called `split_cols`.\n",
    "- Add the following columns based on the first four entries in the variable above: `folder`, `filename`, `width`, `height` on a DataFrame named `split_df`.\n",
    "- Add the `split_cols` variable as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81775f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ca073",
   "metadata": {},
   "source": [
    "### Further parsing\n",
    "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns.\n",
    "\n",
    "The `spark` context is available and `pyspark.sql.functions` is aliased as `F`. The types from `pyspark.sql.types` are already imported. The `split_df DataFrame` is as you last left it. Remember, you can use `.printSchema()` on a DataFrame in the console area to view the column names and types.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a new function called retriever that takes two arguments, the split columns (`cols`) and the total number of columns (`colcount`). This function should return a list of the entries that have not been defined as columns yet (i.e., everything after item 4 in the list).\n",
    "- Define the function as a Spark UDF, returning an Array of strings.\n",
    "- Create the new column `dog_list` using the UDF and the available columns in the DataFrame.\n",
    "- Remove the columns `_c0`, `colcount`, and `split_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2059589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('colcount').drop('split_cols').drop('_c0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f979c6",
   "metadata": {},
   "source": [
    "## Data validation\n",
    "1. Data validation\n",
    "\n",
    "Welcome back! Validation is one step of a data pipeline we haven't covered yet, but it is very important in verifying the quality of the data we're delivering. Let's look at how to implement validation steps in a data cleaning pipeline.\n",
    "\n",
    "2. Definition\n",
    "\n",
    "In this context, validation is verifying that a dataset complies with an expected format. This can include verifying that the number of rows and columns is as expected. For example, is the row count within 2% of the previous month's row count? Another common test is do the data types match? If not specifically validated with a schema, does the content meet the requirements (only 9 characters or less, etc). Finally, you can validate against more complex rules. This includes verifying that the values of a set of sensor readings are within physically possible quantities.\n",
    "\n",
    "3. Validating via joins\n",
    "\n",
    "One technique used to validate data in Spark is using joins to verify the content of a DataFrame matches a known set. Validating via a join will compare data against a set of known values. This could be a list of known ids, companies, addresses, etc. Joins make it easy to determine if data is present in a set. This could be only rows that are in one DataFrame, present in both, or present in neither. Joins are also comparatively fast, especially vs validating individual rows against a long list of entries. The simplest example of this is using an inner join of two DataFrames to validate the data. A new DataFrame, parsed_df, is loaded from a given parquet file. The second DataFrame is loaded containing a list of known company names. A new DataFrame is created by joining parsed_df and company_df on the company name. As this is an inner join, only rows from parsed_df with company names that are present in company_df would be included in the new DataFrame (verified_df). This has the effect of automatically filtering out any rows that don't meet the specified criteria. This is done without any kind of Spark filter or comparison code.\n",
    "\n",
    "4. Complex rule validation\n",
    "\n",
    "Complex rule validation is the idea of using Spark components to validate logic. This may be as simple as using the various Spark calculations to verify the number of columns in an irregular data set. You've done something like this already in the previous lessons. The validation can also be applied against an external source: web service, local files, API calls. These rules are often implemented as a UDF to encapsulate the logic to one place and easily run against the content of a DataFrame.\n",
    "\n",
    "5. Let's practice!\n",
    "\n",
    "Let's try validating our data against our specific requirements for this dataset. Enjoy the exercises and we'll get to the last lesson of this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a7c23",
   "metadata": {},
   "source": [
    "### Validate rows via join\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named `valid_folders_df`. The DataFrame `split_df` is as you last left it with a group of split columns.\n",
    "\n",
    "The `spark` object is available, and `pyspark.sql.functions` is imported as `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Rename the `_c0` column to folder on the `valid_folders_df` DataFrame.\n",
    "- Count the number of rows in `split_df`.\n",
    "- Join the two DataFrames on the folder name, and call the resulting DataFrame `joined_df`. Make sure to broadcast the smaller DataFrame.\n",
    "- Check the number of rows remaining in the DataFrame and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cfcb9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|     _c0|\n",
      "+--------+\n",
      "|02085620|\n",
      "|02085782|\n",
      "|02085936|\n",
      "|02086079|\n",
      "|02086240|\n",
      "|02086646|\n",
      "|02086910|\n",
      "|02087046|\n",
      "|02087394|\n",
      "|02088094|\n",
      "|02088238|\n",
      "|02088364|\n",
      "|02088466|\n",
      "|02088632|\n",
      "|02089078|\n",
      "|02089867|\n",
      "|02090379|\n",
      "|02090622|\n",
      "|02090721|\n",
      "|02091032|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_folders_df = spark.read.csv('valid_folders.csv')\n",
    "valid_folders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e7ac5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 32794\n",
      "After: 20945\n"
     ]
    }
   ],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8f9c3",
   "metadata": {},
   "source": [
    "### Examining invalid rows\n",
    "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.\n",
    "\n",
    "You want to find the difference between two DataFrames and store the invalid rows.\n",
    "\n",
    "The `spark` object is defined and `pyspark.sql.functions` are imported as `F`. The original DataFrame `split_df` and the joined DataFrame `joined_df` are available as they were in their previous states.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Determine the row counts for each DataFrame.\n",
    "- Create a DataFrame containing only the invalid rows.\n",
    "- Validate the count of the new DataFrame is as expected.\n",
    "- Determine the number of distinct folder rows removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24ffbf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " split_df:\t32794\n",
      " joined_df:\t20945\n",
      " invalid_df: \t11849\n",
      "11223 distinct invalid folders found\n"
     ]
    }
   ],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561a501",
   "metadata": {},
   "source": [
    "## Final analysis and delivery\n",
    "1. Final analysis and delivery\n",
    "\n",
    "We've worked with a lot of Spark components while exploring data cleaning. Let's finish up this data cleaning pipeline with some final analysis calculations.\n",
    "\n",
    "2. Analysis calculations (UDF)\n",
    "\n",
    "Analysis calculations are the process of using the columns of data in a DataFrame to compute some useful value using Spark's functionality. We've used UDFs in previous chapters, and this version illustrates calculating an average sale price from a given list of sales. A Python function takes a saleslist argument. For every sale in the saleslist, the function adds the sale entry (from value 2 and 3 in the sale tuple). Once complete, it calculates the actual average per row and returns it. The remaining code is what we've done previously when defining a UDF and using it within a DataFrame.\n",
    "\n",
    "3. Analysis calculations (inline)\n",
    "\n",
    "Spark UDFs are very powerful and flexible and are sometimes the only way to handle certain types of data. Unfortunately UDFs do come at a performance penalty compared to the built-in Spark functions, especially for certain operations. The solution is to perform calculations inline if possible. Spark columns can be defined using in-line math operations, which can then be optimized for the best performance. In this case, we read in a datafile, then have two examples of adding calculated columns. The first is a simple average computed by using two columns in the DataFrame. The second option computes a square footage by multiplying the values in two columns together to create a third. The final line shows the option of mixing a UDF with an inline calculation. There is often a better way to do this, but it does illustrate Spark does not care about the source of the info as long as it conforms to the expected input format.\n",
    "\n",
    "4. Let's practice!\n",
    "\n",
    "Let's finish up this course by performing some analysis on our DataFrame and add meaningful information to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10176142",
   "metadata": {},
   "source": [
    "### Dog parsing\n",
    "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details.\n",
    "\n",
    "The `joined_df` DataFrame is as you last defined it, and the `pyspark.sql.types` have all been imported.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Select the column representing the dog details from the DataFrame and show the first 10 un-truncated rows.\n",
    "- Create a new schema as you've done before, using breed, `start_x`, `start_y`, `end_x`, and `end_y` as the names. Make sure to specify the proper data types for each field in the schema (any number value is an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7d3cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|dog_list                          |\n",
      "+----------------------------------+\n",
      "|[affenpinscher,0,9,173,298]       |\n",
      "|[Border_terrier,73,127,341,335]   |\n",
      "|[kuvasz,0,0,499,327]              |\n",
      "|[Great_Pyrenees,124,225,403,374]  |\n",
      "|[schipperke,146,29,416,309]       |\n",
      "|[groenendael,168,0,469,374]       |\n",
      "|[Bedlington_terrier,10,12,462,332]|\n",
      "|[Lhasa,39,1,499,373]              |\n",
      "|[Kerry_blue_terrier,17,16,300,482]|\n",
      "|[vizsla,112,93,276,236]           |\n",
      "+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce437fc6",
   "metadata": {},
   "source": [
    "### Per image count\n",
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your dog_list column created earlier. You have also created the DogType which will allow better parsing of the data within some of the data columns.\n",
    "\n",
    "The `joined_df` is available as you last defined it, and the `DogType` StructType is defined. `pyspark.sql.functions` is available under the `F` alias.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a Python function to split each entry in `dog_list` to its appropriate parts. Make sure to convert any strings into the appropriate types or the DogType will not parse correctly.\n",
    "- Create a UDF using the above function.\n",
    "- Use the UDF to create a new column called `dogs`. Drop the previous column in the same command.\n",
    "- Show the number of dogs in the new column for the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0754c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|size(dogs)|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "    dogs = []\n",
    "    for dog in doglist:\n",
    "        (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "        dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "    return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05a293",
   "metadata": {},
   "source": [
    "### Percentage dog pixels\n",
    "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
    "\n",
    "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
    "\n",
    "(Xend - Xstart) * (Yend - Ystart)\n",
    "\n",
    "NOTE: You can ignore the possibility of overlapping bounding boxes in this instance.\n",
    "\n",
    "For the percentage, calculate the total number of \"dog\" pixels divided by the total size of the image, multiplied by 100.\n",
    "The `joined_df` DataFrame is as you last used it. `pyspark.sql.functions` is aliased to `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Define a Python function to take a list of tuples (the dog objects) and calculate the total number of \"dog\" pixels per image.\n",
    "- Create a UDF of the function and use it to create a new column called `'dog_pixels'` on the DataFrame.\n",
    "- Create another column, `'dog_percent'`, representing the percentage of `'dog_pixels'` in the image. Make sure this is between 0-100%. Use the string name of the column alone (ie, `\"columnname\"` rather than `df.columnname`).\n",
    "- Show the first 10 rows with more than 60% `'dog_pixels'` in the image. Use a SQL style string for this (ie, `'columnname > ____'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fff2156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
      "|  folder|       filename|width|height|                dogs|dog_pixels|      dog_percent|\n",
      "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
      "|02110627|n02110627_12938|  200|   300|[{affenpinscher, ...|     49997|83.32833333333333|\n",
      "|02104029|   n02104029_63|  500|   375|[{kuvasz, 0, 0, 4...|    163173|          87.0256|\n",
      "|02105056| n02105056_2834|  500|   375|[{groenendael, 16...|    112574|60.03946666666666|\n",
      "|02093647|  n02093647_541|  500|   333|[{Bedlington_terr...|    144640|86.87087087087087|\n",
      "|02098413| n02098413_1355|  500|   375|[{Lhasa, 39, 1, 4...|    171120|           91.264|\n",
      "|02093859| n02093859_2309|  330|   500|[{Kerry_blue_terr...|    131878|79.92606060606062|\n",
      "|02109961| n02109961_1017|  475|   500|[{Eskimo_dog, 43,...|    189189|79.65852631578947|\n",
      "|02108000| n02108000_3491|  600|   450|[{EntleBucher, 30...|    168667|62.46925925925926|\n",
      "|02085782| n02085782_1731|  600|   449|[{Japanese_spanie...|    250125|92.84521158129176|\n",
      "|02110185| n02110185_2736|  259|   500|[{Siberian_husky,...|    113088|87.32664092664093|\n",
      "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 159:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "    totalpixels = 0\n",
    "    for dog in doglist:\n",
    "        totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "    return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "#print(joined_df.show())\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width*joined_df.height)) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.where('dog_percent > 60').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcda7e8",
   "metadata": {},
   "source": [
    "## Congratulations and next steps\n",
    "1. Congratulations and next steps\n",
    "\n",
    "Congratulations! You've successfully completed this course by performing data cleaning operations on several datasets using Python and Apache Spark. While we've touched on many topics, there is a great deal to learn about Spark and how best to perform data cleaning.\n",
    "\n",
    "2. Next Steps\n",
    "\n",
    "To continue your journey with using Apache Spark, there are a few areas I'd advise you to focus on: Reading the Spark documentation is a great way to add to your knowledge and fill in gaps of understanding. Spark is constantly changing and often adds new features without a lot of fanfare. Seasoned Spark developers often find new techniques that removes a lot of complexity from existing code. Spark works on many platforms regardless of size, but it really shines when using it on multi-node clusters with a lot of RAM. You'll be surprised how quickly Spark processes data when given the resources to function as designed. I've personally processed multi-billion row datasets in a few hours on a relatively modest cluster. Finally, I'd suggest working with as many different datasets as you can find. Different types of data require different techniques in Spark and each has challenges when trying to process data in an efficient way. The datasets available within the various courses here on DataCamp are a great place to start.\n",
    "\n",
    "3. Thank you!\n",
    "\n",
    "Thanks and good luck on your journey using Apache Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
