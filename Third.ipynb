{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c55c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:99% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad103c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa2c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      " None \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "hadoop_home = os.environ.get('HADOOP_HOME', None)\n",
    "spark_home = os.environ.get('PYSPARK_PYTHON', None)\n",
    "java_home = os.environ.get('JAVA_HOME', None)\n",
    "print(hadoop_home,'\\n',spark_home,'\\n',java_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1758b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/01 16:15:19 WARN Utils: Your hostname, Alashmony-Lenovo-Z51-70 resolves to a loopback address: 127.0.1.1; using 192.168.1.182 instead (on interface wlp3s0)\n",
      "23/07/01 16:15:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/01 16:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate() \n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1626e720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4dc9f3eb60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d7a97",
   "metadata": {},
   "source": [
    "# 01 DataFrame details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce790d45",
   "metadata": {},
   "source": [
    "## Intro to data cleaning with Apache Spark\n",
    "\n",
    "1. Intro to data cleaning with Apache Spark\n",
    "\n",
    "Welcome to Data Cleaning in Apache Spark with Python. My name is Mike Metzger, I am a Data Engineering Consultant, and I will be your instructor for this course. We will cover what data cleaning is, why it's important, and how to implement it with Spark and Python. Let's get started!\n",
    "\n",
    "2. What is Data Cleaning?\n",
    "\n",
    "In this course, we'll define \"data cleaning\" as preparing raw data for use in processing pipelines. We'll discuss what a pipeline is later on, but for now, it's sufficient to say that data cleaning is a necessary part of any production data system. If your data isn't \"clean\", it's not trustworthy and could cause problems later on. There are many tasks that could fall under the data cleaning umbrella. A few of these include reformatting or replacing text; performing calculations based on the data; and removing garbage or incomplete data.\n",
    "\n",
    "3. Why perform data cleaning with Spark?\n",
    "\n",
    "Most data cleaning systems have two big problems: optimizing performance and organizing the flow of data. A typical programming language (such as Perl, C++, or even standard SQL) may be able to clean data when you have small quantities of data. But consider what happens when you have millions or even billions of pieces of data. Those languages wouldn't be able to process that amount of information in a timely manner. Spark lets you scale your data processing capacity as your requirements evolve. Beyond the performance issues, dealing with large quantities of data requires a process, or pipeline of steps. Spark allows management of many complex tasks within a single framework.\n",
    "\n",
    "4. Data cleaning example\n",
    "\n",
    "Here's an example of cleaning a small data set. We're given a table of names, age in years, and a city. Our requirements are for a DataFrame with first and last name in separate columns, the age in months, and which state the city is in. We also want to remove any rows where the data is out of the ordinary. Using Spark transformations, we can create a DataFrame with these properties and continue processing afterwards.\n",
    "\n",
    "5. Spark Schemas\n",
    "\n",
    "A primary function of data cleaning is to verify all data is in the expected format. Spark provides a built-in ability to validate datasets with schemas. You may have used schemas before with databases or XML; Spark is similar. A schema defines and validates the number and types of columns for a given DataFrame. A schema can contain many different types of fields - integers, floats, dates, strings, and even arrays or mapping structures. A defined schema allows Spark to filter out data that doesn't conform during read, ensuring expected correctness. In addition, schemas also have performance benefits. Normally a data import will try to infer a schema on read - this requires reading the data twice. Defining a schema limits this to a single read operation.\n",
    "\n",
    "6. Example Spark Schema\n",
    "\n",
    "Here is an example schema to the import data from our previous example. First we'll import the pyspark.sql.types library. Next we define the actual StructType list of StructFields, containing an entry for each field in the data. Each StructField consists of a field name, dataType, and whether the data can be null. Once our schema is defined, we can add it into our spark.read.format.load call and process it against our data. The load() method takes two arguments - the filename and a schema. This is where we apply our schema to the data being loaded.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "We've gone over a lot of information regarding data cleaning and the importance of dataframe schemas. Let's put that information to use and practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0293c",
   "metadata": {},
   "source": [
    "### Data cleaning review\n",
    "There are many benefits for using Spark for data cleaning.\n",
    "\n",
    "Which of the following is NOT a benefit?\n",
    "\n",
    "- Spark offers high performance.\n",
    "\n",
    "- **Spark can only handle thousands of records.**\n",
    "\n",
    "- Spark allows orderly data flows.\n",
    "\n",
    "- Spark can use strictly defined schemas while ingesting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c845a",
   "metadata": {},
   "source": [
    "### Defining a schema\n",
    "Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "\n",
    "Name\n",
    "Age\n",
    "City\n",
    "The `Name` and `City` columns are `StringType()` and the `Age` column is an `IntegerType()`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import * from the `pyspark.sql.types` library.\n",
    "- Define a new schema using the `StructType` method.\n",
    "- Define a `StructField` for `name`, `age`, and `city`. Each field should correspond to the correct datatype and not be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12db931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13572b6",
   "metadata": {},
   "source": [
    "## Immutability and lazy processing\n",
    "\n",
    "1. Immutability and Lazy Processing\n",
    "\n",
    "Welcome back! We've had a quick discussion about data cleaning, data types and schemas. Let's move on to some further Spark concepts - Immutability and Lazy Processing.\n",
    "\n",
    "2. Variable review\n",
    "\n",
    "Normally in Python, and most other languages, variables are fully mutable. The values can be changed at any given time, assuming the scope of the variable is valid. While very flexible, this does present problems anytime there are multiple concurrent components trying to modify the same data. Most languages work around these issues using constructs like mutexes, semaphores, etc. This can add complexity, especially with non-trivial programs.\n",
    "\n",
    "3. Immutability\n",
    "\n",
    "Unlike typical Python variables, Spark Data Frames are immutable. While not strictly required, immutability is often a component of functional programming. We won't go into everything that implies here, but understand that Spark is designed to use immutable objects. Practically, this means Spark Data Frames are defined once and are not modifiable after initialization. If the variable name is reused, the original data is removed (assuming it's not in use elsewhere) and the variable name is reassigned to the new data. While this seems inefficient, it actually allows Spark to share data between all cluster components. It can do so without worry about concurrent data objects.\n",
    "\n",
    "4. Immutability Example\n",
    "\n",
    "This is a quick example of the immutability of data frames in Spark. It's OK if you don't understand the actual code, this example is more about the concepts of what happens. First, we create a data frame from a CSV file called voterdata.csv. This creates a new data frame definition and assigns it to the variable name voter_df. Once created, we want to do two further operations. The first is to create a fullyear column by using a 2-digit year present in the data set and adding 2000 to each entry. This does not actually change the data frame at all. It copies the original definition, adds the transformation, and assigns it to the voter_df variable name. Our second operation is similar - now we want to drop the original year column from the data frame. Again, this copies the definition, adds a transformation and reassigns the variable name to this new object. The original objects are destroyed. Please note that the original year column is now permanently gone from this instance, though not from the underlying data (ie, you could simply reload it to a new dataframe if desired).\n",
    "\n",
    "5. Lazy Processing\n",
    "\n",
    "You may be wondering how Spark does this so quickly, especially on large data sets. Spark can do this because of something called lazy processing. Lazy processing in Spark is the idea that very little actually happens until an action is performed. In our previous example, we read a CSV file, added a new column, and deleted another. The trick is that no data was actually read / added / modified, we only updated the instructions (aka, Transformations) for what we wanted Spark to do. This functionality allows Spark to perform the most efficient set of operations to get the desired result. The code example is the same as the previous slide, but with the added count() method call. This classifies as an action in Spark and will process all the transformation operations.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "These concepts can be a little tricky to grasp without some examples. Let's practice these ideas in the coming exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31537d05",
   "metadata": {},
   "source": [
    "### Immutability review\n",
    "You’ve just seen that immutability and lazy processing are fundamental concepts in the way Spark handles data. But why would Spark use immutable data frames to begin with?\n",
    "\n",
    "\n",
    "- To add complexity to your Spark tasks.\n",
    "\n",
    "- **To efficiently handle data throughout the cluster.**\n",
    "\n",
    "- To easily modify variable values as needed.\n",
    "\n",
    "- To conserve storage space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c61745",
   "metadata": {},
   "source": [
    "## Using lazy processing\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (`aa_dfw_df`) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Load the Data Frame.\n",
    "- Add the transformation for `F.lower()` to the Destination Airport column.\n",
    "- Drop the Destination Airport column from the Data Frame `aa_dfw_df`. Note the time for these operations to complete.\n",
    "- Show the Data Frame, noting the time difference for this action to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65882073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2017_Departures_Short.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f410cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39604d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f248dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa8d77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "|       01/01/2017|         0060|                          149|    mia|\n",
      "|       01/01/2017|         0071|                          203|    lax|\n",
      "|       01/01/2017|         0074|                           76|    mem|\n",
      "|       01/01/2017|         0081|                          123|    den|\n",
      "|       01/01/2017|         0089|                          161|    slc|\n",
      "|       01/01/2017|         0096|                           84|    stl|\n",
      "|       01/01/2017|         0103|                          216|    sjc|\n",
      "|       01/01/2017|         0119|                          514|    ogg|\n",
      "|       01/01/2017|         0123|                          529|    hnl|\n",
      "|       01/01/2017|         0126|                          171|    lga|\n",
      "|       01/01/2017|         0132|                          188|    ewr|\n",
      "|       01/01/2017|         0140|                          231|    sjc|\n",
      "|       01/01/2017|         0174|                          145|    rdu|\n",
      "|       01/01/2017|         0176|                          184|    bos|\n",
      "|       01/01/2017|         0190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba834a6",
   "metadata": {},
   "source": [
    "## Understanding Parquet\n",
    "\n",
    "1. Understanding Parquet\n",
    "\n",
    "Welcome back! As we've seen, Spark can read in text and CSV files. While this gives us access to many data sources, it's not always the most convenient format to work with. Let's take a look at a few problems with CSV files.\n",
    "\n",
    "2. Difficulties with CSV files\n",
    "\n",
    "Some common issues with CSV files include: The schema is not defined: there are no data types included, nor column names (beyond a header row). Using content containing a comma (or another delimiter) requires escaping. Using the escape character within content requires even further escaping. The available encoding formats are limited depending on the language used.\n",
    "\n",
    "3. Spark and CSV files\n",
    "\n",
    "In addition to the issues with CSV files in general, Spark has some specific problems processing CSV data. CSV files are quite slow to import and parse. The files cannot be shared between workers during the import process. If no schema is defined, all data must be read before a schema can be inferred. Spark has feature known as predicate pushdown. Basically, this is the idea of ordering tasks to do the least amount of work. Filtering data prior to processing is one of the primary optimizations of predicate pushdown. This drastically reduces the amount of information that must be processed in large data sets. Unfortunately, you cannot filter the CSV data via predicate pushdown. Finally, Spark processes are often multi-step and may utilize an intermediate file representation. These representations allow data to be used later without regenerating the data from source. Using CSV would instead require a significant amount of extra work defining schemas, encoding formats, etc.\n",
    "\n",
    "4. The Parquet Format\n",
    "\n",
    "Parquet is a compressed columnar data format developed for use in any Hadoop based system. This includes Spark, Hadoop, Apache Impala, and so forth. The Parquet format is structured with data accessible in chunks, allowing efficient read / write operations without processing the entire file. This structured format supports Spark's predicate pushdown functionality, providing significant performance improvement. Finally, Parquet files automatically include schema information and handle data encoding. This is perfect for intermediary or on-disk representation of processed data. Note that Parquet files are a binary file format and can only be used with the proper tools. This is in contrast to CSV files which can be edited with any text editor.\n",
    "\n",
    "5. Working with Parquet\n",
    "\n",
    "Interacting with Parquet files is very straightforward. To read a parquet file into a Data Frame, you have two options. The first is using the `spark.read.format` method we've seen previously. The Data Frame, df=spark.read.format('parquet').load('filename.parquet') The second option is the shortcut version: The Data Frame, df=spark.read.parquet('filename.parquet') Typically, the shortcut version is the easiest to use but you can use them interchangeably. Writing parquet files is similar, using either: df.write.format('parquet').save('filename.parquet') or df.write.parquet('filename.parquet') The long-form versions of each permit extra option flags, such as when overwriting an existing parquet file.\n",
    "\n",
    "6. Parquet and SQL\n",
    "\n",
    "Parquet files have various uses within Spark. We've discussed using them as an intermediate data format, but they also are perfect for performing SQL operations. To perform a SQL query against a Parquet file, we first need to create a Data Frame via the spark.read.parquet method. Once we have the Data Frame, we can use the createOrReplaceTempView() method to add an alias of the Parquet data as a SQL table. Finally, we run our query using normal SQL syntax and the spark.sql method. In this case, we're looking for all flights with a duration under 100 minutes. Because we're using Parquet as the backing store, we get all the performance benefits we've discussed previously (primarily defined schemas and the available use of predicate pushdown).\n",
    "\n",
    "7. Let's Practice!\n",
    "\n",
    "You've seen a bit about what a Parquet file is and why we'd want to use them. Now, let's practice working with Parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e0414",
   "metadata": {},
   "source": [
    "## Saving a DataFrame in Parquet format\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it.\n",
    "\n",
    "The spark object and the `df1` and `df2` DataFrames have been setup for you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- View the row count of `df1` and `df2`.\n",
    "- Combine `df1` and `df2` in a new DataFrame named `df3` with the union method.\n",
    "- Save `df3` to a parquet file named `AA_DFW_ALL.parquet`.\n",
    "- Read the `AA_DFW_ALL.parquet` file and show the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a27659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('AA_DFW_2014_Departures_Short.csv.gz', header= True)\n",
    "df2 = spark.read.csv('AA_DFW_2015_Departures_Short.csv.gz', header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1320aa",
   "metadata": {},
   "source": [
    "### If Spark was not able to save as Parquet format, change the winutil version used as Hadoop Home.\n",
    "Mostly, you will need to upgrade WinUtil to match the same Spark version, in my case, Spark 3.4 needs to match Hadoop 3.4. [This repo](https://github.com/kontext-tech/winutils.git) has newer version but not fully trusted. For me, it solved the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247dcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_home = os.environ.get('HADOOP_HOME', None)\n",
    "hadoop_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d206dd70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 157198\n",
      "df2 Count: 146558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303756\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "#rename column to be used in the next cell\n",
    "df3 = df3.withColumnRenamed('Actual elapsed time (Minutes)', 'flight_duration')\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ff654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2014|         0005|                HNL|                          519|\n",
      "|       01/01/2014|         0007|                OGG|                          505|\n",
      "|       01/01/2014|         0035|                SLC|                          174|\n",
      "|       01/01/2014|         0043|                DTW|                          153|\n",
      "|       01/01/2014|         0052|                PIT|                          137|\n",
      "|       01/01/2014|         0058|                SAN|                          174|\n",
      "|       01/01/2014|         0060|                MIA|                          155|\n",
      "|       01/01/2014|         0064|                JFK|                          185|\n",
      "|       01/01/2014|         0090|                ORD|                          126|\n",
      "|       01/01/2014|         0096|                STL|                           91|\n",
      "|       01/01/2014|         0099|                SNA|                          182|\n",
      "|       01/01/2014|         0103|                ONT|                          181|\n",
      "|       01/01/2014|         0109|                DEN|                          127|\n",
      "|       01/01/2014|         0122|                SFO|                          222|\n",
      "|       01/01/2014|         0123|                HNL|                          510|\n",
      "|       01/01/2014|         0129|                COS|                          114|\n",
      "|       01/01/2014|         0130|                DCA|                          141|\n",
      "|       01/01/2014|         0131|                SLC|                          167|\n",
      "|       01/01/2014|         0132|                STL|                           82|\n",
      "|       01/01/2014|         0140|                BWI|                          146|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0439f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2015|         0005|                HNL|                          526|\n",
      "|       01/01/2015|         0007|                OGG|                          517|\n",
      "|       01/01/2015|         0023|                SFO|                          233|\n",
      "|       01/01/2015|         0027|                LAS|                          165|\n",
      "|       01/01/2015|         0029|                ONT|                            0|\n",
      "|       01/01/2015|         0035|                HDN|                          178|\n",
      "|       01/01/2015|         0037|                SAN|                          187|\n",
      "|       01/01/2015|         0043|                DTW|                            0|\n",
      "|       01/01/2015|         0049|                SAN|                          178|\n",
      "|       01/01/2015|         0051|                SLC|                          161|\n",
      "|       01/01/2015|         0060|                MIA|                          151|\n",
      "|       01/01/2015|         0064|                JFK|                          187|\n",
      "|       01/01/2015|         0071|                SAN|                          176|\n",
      "|       01/01/2015|         0072|                MCO|                          142|\n",
      "|       01/01/2015|         0074|                CLE|                            0|\n",
      "|       01/01/2015|         0079|                SMF|                          224|\n",
      "|       01/01/2015|         0081|                TUS|                          140|\n",
      "|       01/01/2015|         0096|                STL|                           94|\n",
      "|       01/01/2015|         0103|                MSY|                           80|\n",
      "|       01/01/2015|         0119|                OGG|                          502|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "848007f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+---------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|flight_duration|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "|       01/01/2014|         0005|                HNL|            519|\n",
      "|       01/01/2014|         0007|                OGG|            505|\n",
      "|       01/01/2014|         0035|                SLC|            174|\n",
      "|       01/01/2014|         0043|                DTW|            153|\n",
      "|       01/01/2014|         0052|                PIT|            137|\n",
      "|       01/01/2014|         0058|                SAN|            174|\n",
      "|       01/01/2014|         0060|                MIA|            155|\n",
      "|       01/01/2014|         0064|                JFK|            185|\n",
      "|       01/01/2014|         0090|                ORD|            126|\n",
      "|       01/01/2014|         0096|                STL|             91|\n",
      "|       01/01/2014|         0099|                SNA|            182|\n",
      "|       01/01/2014|         0103|                ONT|            181|\n",
      "|       01/01/2014|         0109|                DEN|            127|\n",
      "|       01/01/2014|         0122|                SFO|            222|\n",
      "|       01/01/2014|         0123|                HNL|            510|\n",
      "|       01/01/2014|         0129|                COS|            114|\n",
      "|       01/01/2014|         0130|                DCA|            141|\n",
      "|       01/01/2014|         0131|                SLC|            167|\n",
      "|       01/01/2014|         0132|                STL|             82|\n",
      "|       01/01/2014|         0140|                BWI|            146|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25614b",
   "metadata": {},
   "source": [
    "### SQL and Parquet\n",
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file).\n",
    "\n",
    "The spark object and the `AA_DFW_ALL.parquet` file are available for you automatically.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `AA_DFW_ALL.parquet` file into flights_df.\n",
    "- Use the `createOrReplaceTempView` method to alias the flights table.\n",
    "- Run a Spark SQL query against the flights table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32e67bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 143\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7db0e",
   "metadata": {},
   "source": [
    "# Chapter 2: Manipulating DataFrames in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95836c34",
   "metadata": {},
   "source": [
    "## Manipulating DataFrames in the real world\n",
    "1. DataFrame column operations\n",
    "\n",
    "Welcome back! In the first chapter, we've spent some time discussing the basics of Spark data and file handling. Let's now take a look at how to use Spark column operations to clean data.\n",
    "\n",
    "2. DataFrame refresher\n",
    "\n",
    "Before we discuss manipulating DataFrames in depth, let's talk about some of their features. DataFrames are made up of rows & columns and are generally analogous to a database table. DataFrames are immutable: any change to the structure or content of the data creates a new DataFrame. DataFrames are modified through the use of transformations. An example is The .filter() command to only return rows where the name starts with the letter 'M'. Another operation is .select(), in this case returning only the name and position fields.\n",
    "\n",
    "3. Common DataFrame transformations\n",
    "\n",
    "There are many different transformations for use on a DataFrame. They vary depending on what you'd like to do. Some common transformations include: The .filter() clause, which includes only rows that satisfy the requirements defined in the argument. This is analogous to the WHERE clause in SQL. Spark includes a .where() alias which you can use in place of .filter() if desired. This call returns only rows where the vote occurred after 1/1/2019. Another common option is the .select() method which returns the columns requested from the DataFrame. The .withColumn() method creates a new column in the DataFrame. The first argument is the name of the column, and the second is the command(s) to create it. In this case, we create a column called 'year' with just the year information. We also can use the .drop() method to remove a column from a DataFrame.\n",
    "\n",
    "4. Filtering data\n",
    "\n",
    "Among the most common operations used when cleaning a DataFrame, filtering lets us use only the data matching our desired result. We can use .filter() for many tasks, such as: Removing null values. Removing odd entries, anything that doesn't fit our desired format. We can also split a DataFrame containing combined data (such as a syslog file). As mentioned previously, use the .filter() method to return only rows that meet the specified criteria. The .contains() function takes a string argument that the column must have to return true. You can negate these results using the tile (~) character.\n",
    "\n",
    "5. Column string transformations\n",
    "\n",
    "Some of the most common operations used in data cleaning are modifying and converting strings. You will typically apply these to each column as a transformation. Many of these functions are in the pyspark.sql.functions library. For brevity, we'll import it as the alias 'F'. We use the .withColumn() function to create a new column called \"upper\" using pyspark.sql.functions.upper() on the name column. The \"upper\" column will contain uppercase versions of all names. We can create intermediary columns that are only for processing. This is useful to clarify complex transformations requiring multiple steps. In this instance, we call the .split() function with the name of the column and the space character to split on. This returns a list of words in a column called splits. A very common operation is converting string data to a different type, such as converting a string column to an integer. We use the .cast() function to perform the conversion to an IntegerType().\n",
    "\n",
    "6. ArrayType() column functions\n",
    "\n",
    "While performing data cleaning with Spark, you may need to interact with ArrayType() columns. These are analogous to lists in normal python environments. One function we will use is .size(), which returns the number of items present in the specified ArrayType() argument. Another commonly used function for ArrayTypes is .getItem(). It takes an index argument and returns the item present at that index in the list column. Spark has many more transformations and utility functions available. When using Spark in production, make sure to reference the documentation for available options.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "We've discussed some of the common operations used on Spark DataFrame columns. Let's practice some of these now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927f573",
   "metadata": {},
   "source": [
    "### Filtering column content with Python\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame `voter_df` contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the `VOTER_NAME` column.\n",
    "\n",
    "The `pyspark.sql.functions` library is already imported under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Show the distinct `VOTER_NAME` entries.\n",
    "- Filter `voter_df` where the `VOTER_NAME` is 1-20 characters in length.\n",
    "- Filter out `voter_df` where the `VOTER_NAME` contains an `_`.\n",
    "- Show the distinct `VOTER_NAME` entries again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93655d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = spark.read.csv('DallasCouncilVoters.csv.gz', header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c72d31e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District              |\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Michael S. Rawlings                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and  the  methods  of  assessing  special  assessments  on  Dallas  hotels  with    100 or more rooms                                                                                                                           |\n",
      "|Kevin Felder                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Adam Medrano                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Casey  Thomas                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|null                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll  (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District |\n",
      "|011018__42                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Mark  Clayton                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Casey Thomas                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Sandy  Greyson                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Mark Clayton                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Jennifer S.  Gates                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Tiffinni A. Young                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|  the  final  2018 Assessment  Plan   and   the   2018 Assessment   Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing   classifications  for the apportionment of costs and the  methods  of  assessing  special  assessments for the services and improvements  to  property  in  the  District;  closing the hearing and  levying  a  special  assessment  on  property  in  the  District       |\n",
      "|B. Adam  McGough                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|Omar Narvaez                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Philip T. Kingston                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Rickey D. Callahan                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Dwaine R. Caraway                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Philip T.  Kingston                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Jennifer S. Gates                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Lee M. Kleinman                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Monica R. Alonzo                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll   (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District|\n",
      "|Rickey D.  Callahan                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Carolyn King Arnold                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District               |\n",
      "|Erik Wilson                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|  the  final  2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District; closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District                 |\n",
      "|Lee Kleinman                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faa03129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Casey  Thomas      |\n",
      "|Mark  Clayton      |\n",
      "|Casey Thomas       |\n",
      "|Sandy  Greyson     |\n",
      "|Mark Clayton       |\n",
      "|Jennifer S.  Gates |\n",
      "|Tiffinni A. Young  |\n",
      "|B. Adam  McGough   |\n",
      "|Omar Narvaez       |\n",
      "|Philip T. Kingston |\n",
      "|Rickey D. Callahan |\n",
      "|Dwaine R. Caraway  |\n",
      "|Philip T.  Kingston|\n",
      "|Jennifer S. Gates  |\n",
      "|Lee M. Kleinman    |\n",
      "|Monica R. Alonzo   |\n",
      "|Rickey D.  Callahan|\n",
      "|Carolyn King Arnold|\n",
      "|Erik Wilson        |\n",
      "|Lee Kleinman       |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.where('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfe4b7",
   "metadata": {},
   "source": [
    "### Filtering Question #1\n",
    "Consider the following Data Frame called `users_df`:\n",
    "\n",
    "|ID| Name| Age|\tState|\n",
    "|---|---|---|---|\n",
    "|140|\tGeorge| L|\t47|\tIowa|\n",
    "|3260|\tMary| R|\t34|\tVermont|\n",
    "|18502|\tnull|\t68|\tOhio|\n",
    "|999|\tRick| W|\t23|\tCalifornia|\n",
    "\n",
    "If you wanted to return only the entries without nulls, which of following options would **not** work?\n",
    "\n",
    "**Answer the question**\n",
    "**Possible Answers**\n",
    "\n",
    "- users_df = users_df.filter(users_df.Name.isNotNull())\n",
    "\n",
    "- **users_df = users_df.where(users_df.ID == 18502)**\n",
    "\n",
    "- users_df = users_df.where(~ (users_df.ID == 18502) )\n",
    "\n",
    "- users_df = users_df.filter(~ col('Name').isNull())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f31bc",
   "metadata": {},
   "source": [
    "### Modifying DataFrame columns\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - `first_name` and `last_name`. She asks you to split the `VOTER_NAME` column into words on any space character. You'll treat the last word as the `last_name`, and all other words as the `first_name`. You'll be using some new functions in this exercise including `.split()`, `.size()`, and `.getItem()`. The `.getItem(index)` takes an integer value to return the appropriately numbered item in the column. The functions `.split()` and `.size()` are in the `pyspark.sql.functions` library.\n",
    "\n",
    "Please note that these operations are always somewhat specific to the use case. Having your data conform to a format often matters more than the specific details of the format. Rarely is a data cleaning task meant just for one person - matching a defined format allows for easier sharing of the data later (ie, Paul doesn't need to worry about names - Mary already cleaned the dataset).\n",
    "\n",
    "The filtered voter DataFrame from your previous exercise is available as `voter_df`. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Add a new column called splits holding the list of possible names.\n",
    "- Use the `getItem()` method and create a new column called `first_name`.\n",
    "- Get the last entry of the splits list and create a column called `last_name`.\n",
    "- Drop the splits column and show the new `voter_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9b48dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to import again, just aliassing it\n",
    "F = pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb43f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbd5d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alashmony/.local/lib/python3.10/site-packages/pyspark/sql/column.py:458: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits')-1))\n",
    "\n",
    "# Drop the splits column\n",
    "#voter_df = voter_df.drop('splits') #Hashes as we will need it later\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d397bc3",
   "metadata": {},
   "source": [
    "## Conditional DataFrame column operations\n",
    "\n",
    "1. Conditional DataFrame column operations\n",
    "\n",
    "We've looked at some of the power available when using Spark's functions to filter and modify our Data Frames. Let's spend some time with some more advanced options.\n",
    "\n",
    "2. Conditional clauses\n",
    "\n",
    "The DataFrame transformations we've covered thus far are blanket transformations, meaning they're applied regardless of the data. Often you'll want to conditionally change some aspect of the contents. Spark provides some built in conditional clauses which act similar to an if / then / else statement in a traditional programming environment. While it is possible to perform a traditional if / then / else style statement in Spark, it can lead to serious performance degradation as each row of a DataFrame would be evaluated independently. Using the optimized, built-in conditionals alleviates this. There are two components to the conditional clauses: .when(), and the optional .otherwise(). Let's look at them in more depth.\n",
    "\n",
    "3. Conditional example\n",
    "\n",
    "The .when() clause is a method available from the pyspark.sql.functions library that is looking for two components: the if condition, and what to do if it evaluates to true. This is best seen from an example. Consider a DataFrame with the Name and Age columns. We can actually add an extra argument to our .select() method using the .when() clause. We select df.Name and df.Age as usual. For the third argument, we'll define a when conditional. If the Age column is 18 or up, we'll add the string \"Adult\". If the clause doesn't match, nothing is returned. Note that our returned DataFrame contains an unnamed column we didn't define using .withColumn(). The .select() function can create columns dynamically based on the arguments provided. Let's look at some more examples.\n",
    "\n",
    "4. Another example\n",
    "\n",
    "You can chain multiple when statements together, similar to an if / else if structure. In this case, we define two .when() clauses and return Adult or Minor based on the Age column. You can chain as many when clauses together as required.\n",
    "\n",
    "5. Otherwise\n",
    "\n",
    "In addition to .when() is the otherwise() clause. .otherwise() is analogous to the else statement. It takes a single argument, which is what to return, in case the when clause or clauses do not evaluate as True. In this example, we return \"Adult\" when the Age column is 18 or higher. Otherwise, we return \"Minor\". The resulting DataFrame is the same, but the method is different. While you can have multiple .when() statements chained together, you can only have a single .otherwise() per .when() chain.\n",
    "\n",
    "6. Let's practice!\n",
    "\n",
    "Let's try a couple examples of using .when() and .otherwise() to modify some DataFrames!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca701726",
   "metadata": {},
   "source": [
    "### when() example\n",
    "The `when()` clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our `voter_df` DataFrame to add a random number to any voting member that is defined as a \"Councilmember\".\n",
    "\n",
    "The `voter_df` DataFrame is defined and available to you. The `pyspark.sql.functions` library is available as `F`. You can use `F.rand()` to generate the random value.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Add a column to `voter_df` named `random_val` with the results of the `F.rand()` method for any voter with the title Councilmember.\n",
    "- Show some of the DataFrame rows, noting whether the `.when()` clause worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e81336cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.10948335830268863|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|  0.720920387946472|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|               null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|0.14653774208919867|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas| 0.4376974349574557|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.9303114670481522|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs|0.45699271701781485|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough| 0.9881613451127969|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman|  0.355196278189139|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|0.26897442886062106|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.15767741938467073|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston| 0.9402674438659668|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|               null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano| 0.8772362973452061|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas| 0.5595924228591934|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|  0.565724128209479|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan| 0.8807118180811512|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.9939179475299662|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson| 0.8742151593788893|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.8982472185018335|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970db3ef",
   "metadata": {},
   "source": [
    "### When / Otherwise\n",
    "This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your `voter_df` DataFrame to add a random number to any voting member that is defined as a Councilmember. Use 2 for the Mayor and 0 for anything other position.\n",
    "\n",
    "The `voter_df` Data Frame is defined and available to you. The `pyspark.sql.functions` library is available as `F`. You can use `F.rand()` to generate the random value.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Add a column to `voter_df` named `random_val` with the results of the `F.rand()` method for any voter with the title Councilmember. Set `random_val` to 2 for the Mayor. Set any other title to the value 0.\n",
    "- Show some of the Data Frame rows, noting whether the clauses worked.\n",
    "- Use the `.filter clause` to find 0 in `random_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3641e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.40103231355053004|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|  0.292098313673427|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|0.18192386578773923|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas| 0.9851254350409162|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|   0.63992939289795|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs|0.24968408770264017|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough| 0.9031600141301791|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman|0.39442253719656883|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|0.15893160537625894|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.4461558194164216|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|0.14173722938728484|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano| 0.8187098756649076|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|  0.244064211612115|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.6907512446709305|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|0.21726153933706382|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.9655917175378284|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson| 0.4901089753937481|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.3069299625432378|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+-----------------+--------------------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|              splits|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+--------------------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|       Mayor Pro Tem|    Casey  Thomas|     [Casey, Thomas]|     Casey|   Thomas|       0.0|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|06/13/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "|06/13/2018|       Mayor Pro Tem|Dwaine R. Caraway|[Dwaine, R., Cara...|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+--------------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa0412",
   "metadata": {},
   "source": [
    "## User defined functions\n",
    "1. User defined functions\n",
    "\n",
    "We've looked at the built-in functions in Spark and have had great results using these. But let's consider what you would do if you needed to apply some custom logic to your data cleaning processes.\n",
    "\n",
    "2. Defined...\n",
    "\n",
    "A user defined function, or UDF, is a Python method that the user writes to perform a specific bit of logic. Once written, the method is called via the pyspark.sql.functions.udf() method. The result is stored as a variable and can be called as a normal Spark function. Let's look at a couple examples.\n",
    "\n",
    "3. Reverse string UDF\n",
    "\n",
    "Here is a fairly trivial example to illustrate how a UDF is defined. First, we define a python function. We'll call our function, reverseString(), with an argument called mystr. We'll use some python shorthand to reverse the string and return it. Don't worry about understanding how the return statement works, only that it will reverse the lettering of whatever is fed into it (ie, \"help\" becomes \"pleh\"). The next step is to wrap the function and store it in a variable for later use. We'll use the pyspark.sql.functions.udf() method. It takes two arguments - the name of the method you just defined, and the Spark data type it will return. This can be any of the options in pyspark.sql.types, and can even be a more complex type, including a fully defined schema object. Most often, you'll return either a simple object type, or perhaps an ArrayType. We'll call udf with our new method name, and use the StringType(), then store this as udfReverseString(). Finally, we use our new UDF to add a column to the user_df DataFrame within the .withColumn() method. Note that we pass the column we're interested in as the argument to udfReverseString(). The udf function is called for each row of the Data Frame. Under the hood, the udf function takes the value stored for the specified column (per row) and passes it to the python method. The result is fed back to the resulting DataFrame.\n",
    "\n",
    "4. Argument-less example\n",
    "\n",
    "Another quick example is using a function that does not require an argument. We're defining our sortingCap() function to return one of the letters 'G', 'H', 'R', or 'S' at random. We still create our udf wrapped function, and define the return type as StringType(). The primary difference is calling the function, this time without passing in an argument as it is not required.\n",
    "\n",
    "5. Let's practice!\n",
    "\n",
    "As always, the best way to learn is practice - let's create some user defined functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526f9aa",
   "metadata": {},
   "source": [
    "### Understanding user defined functions\n",
    "When creating a new user defined function, which is not a possible value for the second argument?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "- ArrayType(IntegerType())\n",
    "\n",
    "- IntegerType()\n",
    "\n",
    "- LongType()\n",
    "\n",
    "- **udf()**\n",
    "\n",
    "- StringType()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1013aa",
   "metadata": {},
   "source": [
    "### Using user defined functions in Spark\n",
    "You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "For this exercise, we'll use our voter_df DataFrame, but you're going to replace the `first_name` column with the first and middle names.\n",
    "\n",
    "The `pyspark.sql.functions` library is available under the alias `F`. The classes from `pyspark.sql.types` are already imported.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Edit the `getFirstAndMiddle()` function to return a space separated string of names, except the last entry in the names list.\n",
    "- Define the function as a user-defined function. It should return a string type.\n",
    "- Create a new column on `voter_df` called `first_and_middle_name` using your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e77f6de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 89:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|         random_val|first_and_middle_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.40103231355053004|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|  0.292098313673427|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|0.18192386578773923|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas| 0.9851254350409162|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|   0.63992939289795|         Carolyn King|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs|0.24968408770264017|                Scott|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough| 0.9031600141301791|              B. Adam|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman|0.39442253719656883|                  Lee|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|0.15893160537625894|                Sandy|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.4461558194164216|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|0.14173722938728484|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|                2.0|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano| 0.8187098756649076|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|  0.244064211612115|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold| 0.6907512446709305|         Carolyn King|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|0.21726153933706382|            Rickey D.|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.9655917175378284|          Jennifer S.|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson| 0.4901089753937481|                Sandy|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.3069299625432378|          Jennifer S.|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b63c0",
   "metadata": {},
   "source": [
    "## Partitioning and lazy processing\n",
    "1. Partitioning and lazy processing\n",
    "\n",
    "Welcome back to our discussion about modifying and cleaning DataFrames. We've discussed various transformations and methods to modify our data, but we haven't covered much about how Spark actually processes the data. Let's look at that now.\n",
    "\n",
    "2. Partitioning\n",
    "\n",
    "Spark breaks DataFrames into partitions, or chunks of data. These partitions can be automatically defined, enlarged, shrunk, and can differ greatly based on the type of Spark cluster being used. The size of the partition does vary, but generally try to keep your partition sizes equal. We'll discuss more about optimizing partitioning and cluster details later on. For now, let's assume that each partition is handled independently. This is part of what provides the performance levels and horizontal scaling ability in Spark. If a Spark node doesn't need to compete for resources, nor consult with other Spark nodes for answers, it can reliably schedule the processing for the best performance.\n",
    "\n",
    "3. Lazy processing\n",
    "\n",
    "In Spark, any transformation operation is lazy; it's more like a recipe than a command. It defines what should be done to a DataFrame rather than actually doing it. Most operations in Spark are actually transformations, including .withColumn(), .select(), .filter(), and so forth. The set of transformations you define are only executed when you run a Spark action. This includes .count(), .write(), etc - anything that requires the transformations to be run to properly obtain an answer. Spark can reorder transformations for the best performance. Usually this isn't noticeable, but can occasionally cause unexpected behavior, such as IDs not being added until after other transformations have completed. This doesn't actually cause a problem but the data can look unusual if you don't know what to expect.\n",
    "\n",
    "4. Adding IDs\n",
    "\n",
    "Relational databases tend to have a field used to identify the row, whether it is for an actual relationship reference, or just for data identification. These IDs are typically an integer that increases in value, is sequential, and most importantly unique. The problem with these IDs is they're not very parallel in nature. Given that the values are given out sequentially, if there are multiple workers, they must all refer to a common source for the next entry. This is OK in a single server environment, but in a distributed platform such as Spark, it creates some undue bottlenecks. Let's take a look at how to generate ID's in Spark.\n",
    "\n",
    "5. Monotonically increasing IDs\n",
    "\n",
    "Spark has a built-in function called monotonically_increasing_id(), designed to provide an integer ID that increases in value and is unique. These IDs are not necessarily sequential - there can be gaps, often quite large, between values. Unlike a normal relational ID, Spark's is completely parallel - each partition is allocated up to 8 billion IDs that can be assigned. Notice that the ID fields in the sample table are integers, increasing in value, but are not sequential. It's a little out scope, but the IDs are a 64-bit number effectively split into groups based on the Spark partition. Each group contains 8.4 billion IDs, and there are 2.1 billion possible groups, none of which overlap.\n",
    "\n",
    "6. Notes\n",
    "\n",
    "There's a lot of nuance to how partitions and the monotonically increasing ID's work. Remembering that Spark is lazy: it often helps in troubleshooting what can happen. Operations are often out of order - especially if joins are involved. It's best to test your transformations.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "We've discussed a lot of detail in this lesson - let's now take a look at a few exercises that will help solidify how everything works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b71c5",
   "metadata": {},
   "source": [
    "### Adding an ID Field\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset.\n",
    "\n",
    "The spark session and a Spark DataFrame `df` containing the `DallasCouncilVotes.csv.gz` file are available in your workspace. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Select the unique entries from the column `VOTER NAME` and create a new DataFrame called `voter_df`.\n",
    "- Count the rows in the `voter_df` DataFrame.\n",
    "- Add a `ROW_ID` column using the appropriate Spark function.\n",
    "- Show the rows with the 10 highest `ROW_IDs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc8e828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('DallasCouncilVoters.csv.gz',header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10047aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 36 rows in the voter_df DataFrame.\n",
      "\n",
      "+--------------------+------+\n",
      "|          VOTER_NAME|ROW_ID|\n",
      "+--------------------+------+\n",
      "|        Lee Kleinman|    35|\n",
      "|  the  final  201...|    34|\n",
      "|         Erik Wilson|    33|\n",
      "|  the  final   20...|    32|\n",
      "| Carolyn King Arnold|    31|\n",
      "| Rickey D.  Callahan|    30|\n",
      "|   the   final  2...|    29|\n",
      "|    Monica R. Alonzo|    28|\n",
      "|     Lee M. Kleinman|    27|\n",
      "|   Jennifer S. Gates|    26|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER_NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a1fbd",
   "metadata": {},
   "source": [
    "## IDs with different partitions\n",
    "You've just completed adding an ID field to a DataFrame. Now, take a look at what happens when you do the same thing on DataFrames containing a different number of partitions.\n",
    "\n",
    "To check the number of partitions, use the method `.rdd.getNumPartitions()` on a DataFrame.\n",
    "\n",
    "The spark session and two DataFrames, `voter_df` and `voter_df_single`, are available in your workspace. The instructions will help you discover the difference between the DataFrames. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Print the number of partitions on each DataFrame.\n",
    "- Add a `ROW_ID` field to each DataFrame.\n",
    "- Show the top 10 IDs in each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf7ae46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df_single = df\n",
    "voter_df = df\n",
    "voter_df = voter_df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7fd338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 5 partitions in the voter_df DataFrame.\n",
      "\n",
      "\n",
      "There are 1 partitions in the voter_df_single DataFrame.\n",
      "\n",
      "+----------+-------------+-----------------+-----------+\n",
      "|      DATE|        TITLE|       VOTER_NAME|     ROW_ID|\n",
      "+----------+-------------+-----------------+-----------+\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747292|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747291|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747290|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747289|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747288|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747287|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747286|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747285|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747284|\n",
      "|06/28/2017|Mayor Pro Tem|Dwaine R. Caraway|34359747283|\n",
      "+----------+-------------+-----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+--------------------+-------------------+------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|ROW_ID|\n",
      "+----------+--------------------+-------------------+------+\n",
      "|11/20/2018|       Councilmember|      Mark  Clayton| 44624|\n",
      "|11/20/2018|       Councilmember|     Tennell Atkins| 44623|\n",
      "|11/20/2018|       Councilmember|       Kevin Felder| 44622|\n",
      "|11/20/2018|       Councilmember|       Omar Narvaez| 44621|\n",
      "|11/20/2018|       Councilmember|Rickey D.  Callahan| 44620|\n",
      "|11/20/2018|              Vacant|               null| 44619|\n",
      "|11/20/2018|       Mayor Pro Tem|      Casey  Thomas| 44618|\n",
      "|11/20/2018|Deputy Mayor Pro Tem|       Adam Medrano| 44617|\n",
      "|11/20/2018|               Mayor|Michael S. Rawlings| 44616|\n",
      "|11/20/2018|       Councilmember|Philip T.  Kingston| 44615|\n",
      "+----------+--------------------+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461011d9",
   "metadata": {},
   "source": [
    "## More ID tricks\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month.\n",
    "\n",
    "The spark session and two DataFrames, `voter_df_march` and `voter_df_april`, are available in your workspace. The `pyspark.sql.functions` library is available under the alias `F`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Determine the highest `ROW_ID` in `voter_df_march` and save it in the variable `previous_max_ID`. The statement `.rdd.max()[0]` will get the maximum ID.\n",
    "- Add a `ROW_ID` column to `voter_df_april` starting at the value of `previous_max_ID`.\n",
    "- Show the `ROW_ID`'s from both Data Frames and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81898d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|\n",
      "+----------+--------------------+-------------------+\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|\n",
      "|03/21/2018|       Councilmember|Philip T.  Kingston|\n",
      "|03/21/2018|               Mayor|Michael S. Rawlings|\n",
      "|03/21/2018|Deputy Mayor Pro Tem|       Adam Medrano|\n",
      "|03/21/2018|       Councilmember|       Casey Thomas|\n",
      "|03/21/2018|       Mayor Pro Tem|  Dwaine R. Caraway|\n",
      "|03/21/2018|       Councilmember|Rickey D.  Callahan|\n",
      "|03/21/2018|       Councilmember|       Omar Narvaez|\n",
      "|03/21/2018|       Councilmember|       Kevin Felder|\n",
      "|03/21/2018|       Councilmember|     Tennell Atkins|\n",
      "|03/21/2018|       Councilmember|      Mark  Clayton|\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df_march = df.filter(\"(DATE > '02/28/2018') and (DATE < '04/01/2018')\")\n",
    "voter_df_march.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e625439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|\n",
      "+----------+--------------------+-------------------+\n",
      "|04/25/2018|       Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|       Councilmember| Jennifer S.  Gates|\n",
      "|04/25/2018|       Councilmember|Philip T.  Kingston|\n",
      "|04/25/2018|               Mayor|Michael S. Rawlings|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|       Adam Medrano|\n",
      "|04/25/2018|       Councilmember|       Casey Thomas|\n",
      "|04/25/2018|       Mayor Pro Tem|  Dwaine R. Caraway|\n",
      "|04/25/2018|       Councilmember|Rickey D.  Callahan|\n",
      "|04/25/2018|       Councilmember|       Omar Narvaez|\n",
      "|04/25/2018|       Councilmember|       Kevin Felder|\n",
      "|04/25/2018|       Councilmember|     Tennell Atkins|\n",
      "|04/25/2018|       Councilmember|      Mark  Clayton|\n",
      "|04/25/2018|       Councilmember|      Scott  Griggs|\n",
      "|04/25/2018|       Councilmember|   B. Adam  McGough|\n",
      "|04/25/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|04/25/2018|       Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|       Councilmember| Jennifer S.  Gates|\n",
      "|04/25/2018|       Councilmember|   B. Adam  McGough|\n",
      "|04/25/2018|       Councilmember|    Lee M. Kleinman|\n",
      "|04/25/2018|       Councilmember|     Sandy  Greyson|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df_april = df.filter(\"(DATE > '03/31/2018') and (DATE < '05/01/2018')\")\n",
    "voter_df_april.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a80a5422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+------+\n",
      "|      DATE|               TITLE|         VOTER_NAME|ROW_ID|\n",
      "+----------+--------------------+-------------------+------+\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|     0|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|     1|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|     2|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|     3|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|     4|\n",
      "|03/21/2018|       Councilmember|Philip T.  Kingston|     5|\n",
      "|03/21/2018|               Mayor|Michael S. Rawlings|     6|\n",
      "|03/21/2018|Deputy Mayor Pro Tem|       Adam Medrano|     7|\n",
      "|03/21/2018|       Councilmember|       Casey Thomas|     8|\n",
      "|03/21/2018|       Mayor Pro Tem|  Dwaine R. Caraway|     9|\n",
      "|03/21/2018|       Councilmember|Rickey D.  Callahan|    10|\n",
      "|03/21/2018|       Councilmember|       Omar Narvaez|    11|\n",
      "|03/21/2018|       Councilmember|       Kevin Felder|    12|\n",
      "|03/21/2018|       Councilmember|     Tennell Atkins|    13|\n",
      "|03/21/2018|       Councilmember|      Mark  Clayton|    14|\n",
      "|03/21/2018|       Councilmember|      Scott  Griggs|    15|\n",
      "|03/21/2018|       Councilmember|   B. Adam  McGough|    16|\n",
      "|03/21/2018|       Councilmember|    Lee M. Kleinman|    17|\n",
      "|03/21/2018|       Councilmember|     Sandy  Greyson|    18|\n",
      "|03/21/2018|       Councilmember| Jennifer S.  Gates|    19|\n",
      "+----------+--------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df_march = voter_df_march.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_march.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "620a5fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|  1109|\n",
      "|  1110|\n",
      "|  1111|\n",
      "|  1112|\n",
      "|  1113|\n",
      "|  1114|\n",
      "|  1115|\n",
      "|  1116|\n",
      "|  1117|\n",
      "|  1118|\n",
      "|  1119|\n",
      "|  1120|\n",
      "|  1121|\n",
      "|  1122|\n",
      "|  1123|\n",
      "|  1124|\n",
      "|  1125|\n",
      "|  1126|\n",
      "|  1127|\n",
      "|  1128|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f580b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
